{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc612d54-3e46-43df-9911-7c095dfb2cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e76b8d0a-375e-4707-9753-e2b56f5ebf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "from collections import namedtuple\n",
    "from typing import Iterable, Literal\n",
    "from copy import deepcopy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fd3d705-1332-421e-bc79-27c46af31aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97a1ae74-4c90-417a-bb97-c9fd4f9f2dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3488869e-97e0-42e4-acb5-b1e0500bac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"x-avenue-450615-c3\"\n",
    "dataset_name = \"jobs_postings\"\n",
    "location = \"europe-west1\"\n",
    "\n",
    "bq_client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da6bec10-a418-4d82-81aa-d2ac345c983f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/repos/jobs-research/pipelines/rapidapi_jobs_posting was added to sys.path\n",
      "/home/jovyan/work/repos/jobs-research was added to sys.path\n"
     ]
    }
   ],
   "source": [
    "CURRENT_DIRECTORY = os.getcwd()\n",
    "PROJECT_ROOT = os.path.abspath(\n",
    "    os.path.join(CURRENT_DIRECTORY, \"../pipelines/rapidapi_jobs_posting/\")\n",
    ")\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "    print(f\"{PROJECT_ROOT} was added to sys.path\")\n",
    "\n",
    "REPO_ROOT = os.path.abspath(os.path.join(CURRENT_DIRECTORY, \"..\"))\n",
    "if REPO_ROOT not in sys.path:\n",
    "    sys.path.append(REPO_ROOT)\n",
    "    print(f\"{REPO_ROOT} was added to sys.path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38e56b66-d964-4f7a-b27f-5dde208086db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.utils import bq_table_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5332e104-eabd-4a0f-bb2f-b035c59ba44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_url = \"https://docs.google.com/spreadsheets/d/1clAiWIVMD5bCJRHJr9-p2vw9h99W5sByAtqThIGREpo/edit?gid=0#gid=0\"\n",
    "csv_export_url = sheet_url.replace(\"/edit?gid=\", \"/export?format=csv&gid=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e2848be0-868a-4220-ba36-0c580bc65420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://docs.google.com/spreadsheets/d/1clAiWIVMD5bCJRHJr9-p2vw9h99W5sByAtqThIGREpo/export?format=csv&gid=0#gid=0'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_export_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "bf4901fb-0f8c-4ab9-9058-51f35bbdb8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_df = pd.read_csv(csv_export_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "ae28c9f8-8460-4728-a862-528e69ee2f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 113 entries, 0 to 112\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   keyword           37 non-null     object\n",
      " 1   result            113 non-null    object\n",
      " 2   case_sensitive    113 non-null    bool  \n",
      " 3   spaces_sensitive  113 non-null    bool  \n",
      "dtypes: bool(2), object(2)\n",
      "memory usage: 2.1+ KB\n"
     ]
    }
   ],
   "source": [
    "rules_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "2681f1d5-632f-43b2-a79f-d7e03755b4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Google Cloud Platform': {'Google BigQuery',\n",
       "  'Google Cloud Platform',\n",
       "  'Google Cloud Run',\n",
       "  'Google Looker Studio',\n",
       "  'Google Storage',\n",
       "  'Google Vertex AI'},\n",
       " 'Microsoft Azure': {'Azure Blob Storage',\n",
       "  'Azure Cosmos',\n",
       "  'Azure Data Factory',\n",
       "  'Azure Function',\n",
       "  'Azure Synapse Analytics',\n",
       "  'Microsoft Azure'},\n",
       " 'Amazon Web Services': {'Amazon Athena',\n",
       "  'Amazon Firehose',\n",
       "  'Amazon Glue',\n",
       "  'Amazon Redshift',\n",
       "  'Amazon S3',\n",
       "  'Amazon Web Services'}}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_set = rules_df.result.unique()\n",
    "\n",
    "cloud_skills_sets = dict()\n",
    "\n",
    "cloud_skills_sets[\"Google Cloud Platform\"] = {x for x in skills_set if \"Google\" in x}\n",
    "cloud_skills_sets[\"Microsoft Azure\"] = {x for x in skills_set if \"Azure\" in x}\n",
    "cloud_skills_sets[\"Amazon Web Services\"] = {x for x in skills_set if \"Amazon\" in x}\n",
    "\n",
    "cloud_skills_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "eee4a0a0-8fae-4176-b1ff-822b3f387f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_WITH_SPACES = r\"[!\\\"$\\%'()\\+,\\-./:;?]\"\n",
    "\n",
    "\n",
    "def prepare_text(\n",
    "    text: str,\n",
    "    case_sensitive: bool,\n",
    "    spaces_sensitive: bool,\n",
    "    replace_with_spaces=REPLACE_WITH_SPACES,\n",
    ") -> str:\n",
    "    if not case_sensitive:\n",
    "        text = text.lower()\n",
    "    if not spaces_sensitive:\n",
    "        text = text.replace(\" \", \"\")\n",
    "    else:\n",
    "\n",
    "        text = re.sub(replace_with_spaces, \" \", text)\n",
    "        text = \" \" + text.strip() + \" \"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a648c6d6-c8d8-4257-a784-5f3589312158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappingDict:\n",
    "    def __init__(\n",
    "        self,\n",
    "        rules: dict,\n",
    "        case_sensitive: bool = False,\n",
    "        spaces_sensitive: bool = False,\n",
    "    ):\n",
    "        self.rules = rules\n",
    "        self.case_sensitive = case_sensitive\n",
    "        self.spaces_sensitive = spaces_sensitive\n",
    "        self.is_prepared = False\n",
    "\n",
    "    def prepare(self):\n",
    "        if not self.is_prepared:\n",
    "            prepared_rules = {}\n",
    "            for key, val in self.rules.items():\n",
    "                prepared_key = prepare_text(\n",
    "                    key, self.case_sensitive, self.spaces_sensitive\n",
    "                )\n",
    "                prepared_rules[prepared_key] = val\n",
    "                for char in REPLACE_WITH_SPACES:\n",
    "                    if self.spaces_sensitive and prepared_key != key and char in key:\n",
    "                        warnings.warn(\n",
    "                            f\"'{char}' in the keyword '{key}' was replaced with a space.\",\n",
    "                            UserWarning,\n",
    "                        )\n",
    "            self.rules = prepared_rules\n",
    "            self.is_prepared = True\n",
    "        else:\n",
    "            warnings.warn(\"MappingDict is already prepared.\", UserWarning)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "8a0d3d92-38db-4b7e-91d7-854cb61abe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappingRules:\n",
    "    def __init__(self, attr_name: str, rules_df: pd.DataFrame):\n",
    "        self.attr_name = attr_name\n",
    "        self.rules_df = rules_df[\n",
    "            [\"keyword\", \"result\", \"case_sensitive\", \"spaces_sensitive\"]\n",
    "        ]\n",
    "\n",
    "        self._check_for_nulls()\n",
    "        self.rules_df[\"keyword\"] = self.rules_df.apply(\n",
    "            lambda x: x.result if pd.isna(x.keyword) else x.keyword, axis=\"columns\"\n",
    "        )\n",
    "        self._check_keywords_uniqueness()\n",
    "        self._is_prepared = False\n",
    "\n",
    "    def _check_for_nulls(self):\n",
    "        for col in [\"result\", \"case_sensitive\", \"spaces_sensitive\"]:\n",
    "            if rules_df[col].isna().sum() > 0:\n",
    "                raise ValueError(\n",
    "                    f\"'{col}' can not be Null. Fix mapping rules for {self.attr_name}.\"\n",
    "                )\n",
    "\n",
    "    def _check_keywords_uniqueness(self):\n",
    "        non_unique_keywords = self.rules_df[\"keyword\"].value_counts()[lambda x: x > 1]\n",
    "        if len(non_unique_keywords) > 0:\n",
    "            non_unique_keywords_str = \", \".join(non_unique_keywords.index)\n",
    "            raise ValueError(\n",
    "                f\"Keywords must be unique. Fix mapping rules for {self.attr_name}: {non_unique_keywords_str}.\"\n",
    "            )\n",
    "\n",
    "    def prepare(self):\n",
    "        if not self._is_prepared:\n",
    "            self.map_dicts = list(\n",
    "                self.rules_df.set_index(\"keyword\")\n",
    "                .groupby(by=[\"case_sensitive\", \"spaces_sensitive\"])\n",
    "                .agg(dict)\n",
    "                .rename({\"result\": \"mapping_dict\"}, axis=\"columns\")\n",
    "                .reset_index()\n",
    "                .apply(\n",
    "                    lambda x: MappingDict(\n",
    "                        rules=x.mapping_dict,\n",
    "                        case_sensitive=x.case_sensitive,\n",
    "                        spaces_sensitive=x.spaces_sensitive,\n",
    "                    ),\n",
    "                    axis=\"columns\",\n",
    "                )\n",
    "            )\n",
    "            self.map_dicts_prepared = [\n",
    "                mapping_dict.prepare() for mapping_dict in self.map_dicts\n",
    "            ]\n",
    "            self._is_prepared = True\n",
    "        else:\n",
    "            warnings.warn(\"MappingRules are already prepared.\", UserWarning)\n",
    "\n",
    "    def apply(\n",
    "        self, texts: Iterable[str], find: Literal[\"any\", \"all\"] = \"all\"\n",
    "    ) -> str | set[str] | None:\n",
    "\n",
    "        if not self._is_prepared:\n",
    "            self.prepare()\n",
    "\n",
    "        if find == \"all\":\n",
    "            result = set()\n",
    "        if find == \"any\":\n",
    "            result = None\n",
    "\n",
    "        for mapping_dict in self.map_dicts_prepared:\n",
    "            for text in texts:\n",
    "                text = prepare_text(\n",
    "                    text, mapping_dict.case_sensitive, mapping_dict.spaces_sensitive\n",
    "                )\n",
    "                for key, val in mapping_dict.rules.items():\n",
    "                    if key in text:\n",
    "                        if find == \"all\":\n",
    "                            result.add(val)\n",
    "                        if find == \"any\":\n",
    "                            return val\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "1127ddb9-28ce-41c9-b19c-48332ca4c861",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_rules = MappingRules(\"skills\", rules_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "0f5b1ff7-f246-442d-86a8-01f949e2b143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BI'}"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_rules.apply([\"BI\"], \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "77e58462-b5bd-4bda-8f7e-37419fe20056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BI'}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_rules.apply([\" BI \"], \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "533aa010-8727-4a74-8178-132153f67356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BI'}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_rules.apply([\"hI BI!\"], \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6fbc55e0-0f34-4bb0-beec-52f5eafa6989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_rules.apply([\" bi \"], \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9ba461e2-bee2-4150-8834-51f89fd898d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_rules.apply([\" BINGO!\"], \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "fec59903-a1a0-4cee-a032-3ecfd9bc61a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Google BigQuery'}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_rules.apply([\"BigQuery\"], \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "286b7d91-44e5-49ea-b961-ceefdc3d2533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Google BigQuery'}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_rules.apply([\"(BigQuery)\"], \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "ce826a6f-ab69-4f91-b67e-71b061bcb0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Google BigQuery'}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_rules.apply([\"(Big Query)\"], \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "1f41d0ce-5b92-4b76-b7e5-ccd675590909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'JavaScript'}"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_rules.apply([\"(JavaScRiPt \"], \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "4b32cd2d-646d-4216-994f-bae15dffe15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Java'}"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_rules.apply([\"(Java ScRiPt \"], \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "6e30c2e0-6bb6-44ed-9e45-f8aa4b33b22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Azure Cosmos', 'Google BigQuery', 'Hive'}"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_rules.apply([\"piu-piu(bigquery/Hive/CosmosDB)\"], \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e8f3676-d266-4a90-aa08-7a3a7b9eb076",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posting_load_query = f\"\"\"\n",
    "select *\n",
    "from `x-avenue-450615-c3.jp.jobs` as jp\n",
    "\"\"\"\n",
    "df_posting = bq_client.query(df_posting_load_query).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "9f9fcb82-051c-4c09-ab39-c9c66a5a45ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_posting[\"skills\"] = df_posting.description.map(\n",
    "    lambda x: set() if pd.isna(x) else skills_rules.apply([x])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "89f6bb83-f9d4-4f16-9902-2344cd9a3ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "position\n",
       "Data Scientist                           877\n",
       "Data Analyst                             779\n",
       "Data Engineer                            592\n",
       "Data Protection/Governance Specialist    139\n",
       "Architect                                134\n",
       "Consultant/Advisor                       100\n",
       "Software Engineer                         82\n",
       "Data Manager                              65\n",
       "Data Entry Specialist                     54\n",
       "IaC Specialist                            50\n",
       "Project Manager                           47\n",
       "Product Manager                           46\n",
       "Data Quality Specialist                   36\n",
       "Facility Engineer                         30\n",
       "Product Owner                             24\n",
       "ML Ops                                     9\n",
       "Tutor/Teacher                              2\n",
       "Network Engineer                           1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posting.position.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "d8a733ba-16cd-4e5b-9600-0aa3e3bfc3fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Python', 'ML', 'SQL', 'Deep learning', 'PostgreSQL', 'Azure'} \n",
      "\n",
      " What we are doing\n",
      "\n",
      "Our mission is to prevent vision loss and ultimately blindness by developing AI software that assists eye doctors in therapy planning for their patients. We use computer vision deep learning models(AI) trained on thousands of cases and millions of images to identify and predict disease progression. The decision support algorithm targets common eye diseases like Age-related Macular Degeneration (AMD, 7.5mio affected in Germany alone).\n",
      "\n",
      "Tasks {#tasks}\n",
      "- -------------\n",
      "\n",
      "Your responsibilities\n",
      "\n",
      "* You are responsible for building and optimizing data pipelines to bring together information from different source systems, i.e. imaging and clinical data\n",
      "* You prepare data for analytical or operational uses, i.e. by integration, consolidation, cleansing, and structuring of data\n",
      "* You bring curiosity, problem-solving instinct and statistical and mathematical knowledge.\n",
      "\n",
      "Requirements {#requirements}\n",
      "- ---------------------------\n",
      "\n",
      "What we are looking for\n",
      "\n",
      "Your must-haves\n",
      "\n",
      "* Experience in data engineering related to computer vision and imaging analysis (2D/3D image processing, segmentation, image normalization, ideally medical imaging)\n",
      "* Applied knowledge of deep and machine learning (ML) techniques\n",
      "* Experience in programming in Python, in developing software in AI and data-centric domains\n",
      "\n",
      "Your nice-to-haves\n",
      "\n",
      "* Studies in data science/engineering, bioengineering, computer science, medical physics or similar\n",
      "* Experience in working with relational databases (e.g. PostgreSQL), and modern, scalable storage technologies (e.g. Azure FileShare)\n",
      "* Working-level English (C1 or higher)\n",
      "\n",
      "Benefits {#benefits}\n",
      "- -------------------\n",
      "\n",
      "What we offer\n",
      "\n",
      "* Direct impact on improving quality of life for patients \\& doctors\n",
      "* High personal responsibility for your project \\& fast personal growth\n",
      "* Flexible working hours, remote work \\& office at Munich main station\n",
      "* Startup culture \\& an international rock star team\n",
      "\n",
      "Reach out to us with information about your availability, salary expectations and a link to your conclusive profile (GitHub, LinkedIn, CV). Please share with us why you want to help us in preventing blindness with AI. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'Hadoop', 'Python', 'Cloud', 'Google Cloud Platform', 'Amazon Web Services', 'Spark', 'SQL', 'Microsoft Excel', 'Azure'} \n",
      "\n",
      " Job Title: Junior Data Engineer\n",
      "\n",
      "Location: \\[Insert Location or Remote\\]\n",
      "\n",
      "Job Type: Full-time\n",
      "\n",
      "Job Description:\n",
      "\n",
      "We are looking for a motivated and detail-oriented Junior Data Engineer to join our growing data team. In this role, you will assist in building and maintaining scalable data pipelines, ensuring data integrity, and supporting data-driven decision-making across the organization. This is an excellent opportunity for someone early in their career who is passionate about data and eager to grow in a dynamic environment.\n",
      "\n",
      "Responsibilities:\n",
      "\n",
      "* Assist in the design, development, and maintenance of reliable data pipelines and ETL processes\n",
      "* Help collect, clean, and organize large datasets from multiple sources\n",
      "* Support data integration and migration efforts across platforms\n",
      "* Work closely with data scientists, analysts, and software engineers to ensure data availability and quality\n",
      "* Troubleshoot and resolve data-related issues in a timely manner\n",
      "* Maintain documentation related to data pipelines, structures, and processes\n",
      "* Continuously learn and stay updated on industry best practices and new technologies\n",
      "\n",
      "Requirements:\n",
      "\n",
      "* Bachelor's degree in Computer Science, Data Engineering, Information Systems, or a related field (or equivalent experience)\n",
      "* Basic knowledge of SQL and relational databases\n",
      "* Familiarity with Python or other programming languages used for data processing\n",
      "* Understanding of ETL concepts and data warehousing principles\n",
      "* Strong analytical and problem-solving skills\n",
      "* Good communication skills and ability to work collaboratively\n",
      "\n",
      "Nice to Have:\n",
      "\n",
      "* Experience with cloud platforms (e.g., AWS, Azure, Google Cloud)\n",
      "* Familiarity with big data tools (e.g., Spark, Hadoop)\n",
      "* Knowledge of data modeling and database design\n",
      "* Internship or hands-on project experience in data engineering\n",
      "\n",
      "What We Offer:\n",
      "\n",
      "* A collaborative and innovative work environment\n",
      "* Mentorship and professional development opportunities\n",
      "* Competitive salary and benefits package\n",
      "* Remote or hybrid working options depending on project needs \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'Python', 'LLM', 'Java', 'JavaScript', 'Microsoft Excel'} \n",
      "\n",
      " Outlier helps the world's most innovative companies improve their AI models by providing human feedback. Are you an experienced software engineer who would like to lend your coding expertise to train AI models?  \n",
      "\n",
      "We partner with organizations to train AI large language models, helping cutting-edge generative AI models write better code. Projects typically include discrete, highly variable problems that involve engaging with these models as they learn to code. There is no requirement for previous AI experience.  \n",
      "\n",
      "**You must write German for this specific AI training opportunity** **About The Opportunity**\n",
      "\n",
      "* Outlier is looking for talented coders that also speak German to help train generative artificial intelligence models\n",
      "* This freelance opportunity is remote and hours are flexible, so you can work whenever is best for you  \n",
      "\n",
      "**You may contribute your expertise by...**\n",
      "\n",
      "* Crafting and answering questions related to computer science in order to help train AI models\n",
      "* Evaluating and ranking code generated by AI models  \n",
      "\n",
      "**Examples Of Desirable Expertise**\n",
      "\n",
      "* Ability to articulate complex concepts fluently in German (required)\n",
      "* Currently enrolled in or completed a bachelor's degree or higher in computer science at a selective institution\n",
      "* Proficiency working with one or more of the the following languages: Java, Python, JavaScript / TypeScript, C++, Swift, and Verilog\n",
      "* Excellent attention to detail, including grammar, punctuation, and style guidelines  \n",
      "\n",
      "**Payment**\n",
      "\n",
      "* Currently, pay rates for core project work by coding experts range from USD $25 to $50 per hour.\n",
      "* Rates vary based on expertise, skills assessment, location, project need, and other factors. For example, higher rates may be offered to PhDs. For non-core work, such as during initial project onboarding or project overtime phases, lower rates may apply. Certain projects offer incentive payments. Please review the payment terms for each project.  \n",
      "\n",
      "*PLEASE NOTE: We collect, retain and use personal data for our professional business purposes, including notifying you of opportunities that may be of interest and sharing with our affiliates. We limit the personal data we collect to that which we believe is appropriate and necessary to manage applicants' needs, provide our services, and comply with applicable laws. Any information we collect in connection with your application will be treated in accordance with the* *Outlier Privacy Policy* *and our internal policies and programs designed to protect personal data.* *This is a 1099 contract opportunity on the* *Outlier.ai* *platform. Because this is a freelance opportunity, we do not offer internships, sponsorship, or employment. You must be authorized to work in your country of residence. If you are an international student, you may be able to sign up for Outlier if you are on a visa. You should contact your tax and/or immigration advisor with specific questions regarding your circumstances.* \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'Python', 'ML', 'Grafana', 'NumPy', 'Kubernetes', 'Spark', 'Trino', 'SQL', 'Microsoft Excel', 'PySpark'} \n",
      "\n",
      " **Your Tasks**\n",
      "\n",
      "Millions of customers use our brands GMX, WEB.DE, 1\\&1 and mail.com every day, generating billions of requests. Our team develops systems to detect and prevent identity and resource misuse. You will work with experienced architects and developers, contributing with your initiative and independent work style.\n",
      "\n",
      "* Develop, test, deploy, and monitor scalable filter pipelines and transformation processes that extract relevant metrics and features from billions of events.\n",
      "* Data management, versioning, etc., in a Data Lake.\n",
      "* Implement high-performance ML inference services for critical applications.\n",
      "* Collaborate with data scientists and software developers to optimize and scale ML models.\n",
      "* Ensure the quality and reliability of developed solutions through monitoring and performance analysis.\n",
      "\n",
      "**Your Profile**\n",
      "\n",
      "Do you have a degree in computer science, mathematics, statistics, or comparable professional experience?\n",
      "\n",
      "* You have several years of experience as a backend software developer.\n",
      "* You have excellent programming skills in Python, as well as experience with PySpark, SQL/Trino, Numpy.\n",
      "* You have expertise in handling large datasets and distributed systems.\n",
      "* You have experience with Gitlab/deployment pipelines and Kubernetes, monitoring systems like Grafana and Prometheus are a plus.\n",
      "\n",
      "**Our Benefits**\n",
      "\n",
      "* Our corporate culture: No dress code, flat hierarchies, open and transparent communication\n",
      "* Individual development opportunities: diverse training courses, e-learning and internal communities, language courses, mentoring\n",
      "* Events: Slack Days, open source projects, meet-ups\n",
      "* Relocation service: support with the relocation to Germany\n",
      "* Benefits and additional services: company pension scheme, capital-forming benefits, discounts on own products, job ticket, bike leasing, corporate benefits portal\n",
      "* Attractive working conditions: 30 days holiday, hybrid working, full-time and part-time arrangements, free choice between Linux, Mac or Windows\n",
      "* Social: team events, summer and winter parties, family and care service, sports and fitness programmes, subsidised canteen, free fruit and drinks, health courses\n",
      "* Topics that are also important to us: Sustainability, diversity and our values and leadership principles - find out more on our website mail-and-media.com\n",
      "\n",
      "**Reference ID: 263** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'NoSQL', 'SQL', 'RDBMS'} \n",
      "\n",
      " **Data Engineer - Sagemaker/Jupyter, and data lake, NoSql, RDBMS, ETL**\n",
      "\n",
      "Duration: 6 months extendable\n",
      "\n",
      "Languages: German \\& English fluency\n",
      "\n",
      "Work mode: Remote working with occasional travel to Frankfurt\n",
      "\n",
      "An experienced Data Engineer is required for a long-term project with a fast-growing technology company based in Germany.\n",
      "\n",
      "* 4-6 years of experience in various data architecture and engineering roles within data \\& analytics\n",
      "* Collaborate with stakeholders to understand and document data requirements, business rules, and objectives for the data platform.\n",
      "* Design and develop conceptual, logical, and physical data models that accurately represent the organization's data assets and support its business needs.\n",
      "* Ensure designs meet documented objectives for reliability, scalability, supportability, user experience, security, governance, performance and more\n",
      "* Implement data engineering practices, including normalization, denormalization, and indexing, to ensure data integrity, performance, and scalability.\n",
      "* Work closely with architects to integrate data engineering into the overall platform architecture, ensuring efficient data processing and storage.\n",
      "* Communicate effectively with technical and non-technical stakeholders to present and explain data engineering, design decisions, and recommendations.\n",
      "* 3 years of hands-on relational, dimensional, and/or analytic experience (using RDBMS, dimensional, NoSQL data platform technologies, and ETL and data ingestion protocols).\n",
      "* Knowledge of Sagemaker/Jupyter, and data lake\n",
      "* Experience with data warehouse, data lake, and enterprise big data platforms in multi-data-center contexts required.\n",
      "* Implement business and IT data requirements through new data strategies and designs across all data platforms (relational, dimensional, and NoSQL) and data tools (reporting, visualization, analytics, and machine learning).\n",
      "* Identify the architecture, infrastructure, and interfaces to data sources, tools supporting automated data loads, security concerns, analytic models, and data visualization.\n",
      "* Hands-on modeling, design, configuration, installation, performance tuning, and sandbox POC\n",
      "* Ensuring that data are modelled and processed according to architecture and requirements both functional and non-functional\n",
      "* Understanding and implementing required development guidelines, design standards and best practices\n",
      "* Delivering right solution architecture, automation and technology choices\n",
      "* Working cross-functionally with enterprise architects, information security teams, and platform teams\n",
      "* Suggesting and implementing architecture improvements. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'Python', 'TensorFlow', 'PyTorch', 'SQL', 'Azure Synapse Analytics', 'BI', 'PowerBI', 'Azure'} \n",
      "\n",
      " Job description\n",
      "===============\n",
      "\n",
      "**Join us in shaping the future of sustainable data solutions!**   \n",
      "\n",
      "Are you passionate about data? Do you want to work with innovative technologies while making a positive contribution to the environment? Then you've come to the right place!   \n",
      "\n",
      "iPoint-systems is a leading company in the field of sustainability software. We help international companies to make optimal use of their data for environmental analysis, life cycle assessments, and ESG reporting.   \n",
      "\n",
      "To strengthen our team in Hamburg, we are looking for a Data Engineer (m/f/d) with a passion for data, AI, and sustainability.\n",
      "\n",
      "Your profile\n",
      "============\n",
      "\n",
      "* Degree in data science, computer science, mathematics or environmental science with a focus on data\n",
      "* At least 2 years of professional experience or relevant student trainee work\n",
      "* Very good knowledge of Python \\& SQL (must-have)\n",
      "* Experience in machine learning \\& frameworks (e.g. sklearn, TensorFlow, PyTorch)\n",
      "* Ideally experience with GenAI \\& Microsoft Azure (Fabric, Synapse, Power BI)\n",
      "* Ability to work independently \\& show initiative (little senior supervision)\n",
      "* Team player with strong communication skills\n",
      "* Interest in sustainability \\& innovative data solutions\n",
      "* Very good English skills required, German an advantage\n",
      "\n",
      "Your mission\n",
      "============\n",
      "\n",
      "* Developing AI-supported sustainability solutions\n",
      "* Building \\& optimizing data pipelines for customer data\n",
      "* Generating reports \\& dashboards for data visualization\n",
      "* Modeling \\& training with Umberto 11 for sustainability analysis\n",
      "* Working closely with architects, developers \\& customers\n",
      "* Pair programming and technical exchange within the team\n",
      "* Customer consulting in the areas of data connection \\& software training\n",
      "\n",
      "iPoint\n",
      "======\n",
      "\n",
      "Why iPoint is the best choice for you\n",
      "=====================================\n",
      "\n",
      "* **Trusted by Global Brands** For over 20 years, we've been partnering with major international companies.\n",
      "* **Compliance \\& Sustainability -- We Master Both**What sets us apart? We seamlessly combine compliance and sustainability.\n",
      "* **Work Where it Works Best for You** We offer modern offices at multiple locations, but you can also work remotely or even abroad, thanks to our flexible working models.\n",
      "* **A Supportive \\& Inspiring Work Environment**Enjoy the perfect mix of collaboration and independence in a team-oriented culture.\n",
      "* **We Value You** Benefit from a competitive salary package, including a company pension plan and health perks.\n",
      "* **We Celebrate Success Together** Regular team and company events help us stay connected and strengthen our community.\n",
      "* **More Benefits** Enjoy 30 days of vacation, free drinks, fresh fruit, and JobRad leasing - so you can stay active and commute sustainably!\n",
      "\n",
      "Bildauswahl\n",
      "=========== \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'Python', 'Terraform', 'Databricks', 'Spark', 'Batch processing', 'SQL', 'Azure'} \n",
      "\n",
      " **Senior Data Engineer \\| Berlin (2 days per week) \\| Databricks -- Azure \\| up to €100k**\n",
      "\n",
      "AI Futures has been exclusively retained to find a Data Engineer for a market leading manufacturing organisation with annual revenues of over €5 Billion. They are at the start of their digital transformation journey and are transforming their way of working with data-driven decision making to provide sustainable solutions in the manufacturing industry. This role requires two days per week on-site in Berlin and offers a package of up to €100K.\n",
      "\n",
      "The company fosters a continuous learning culture with personalized training programs, leadership development, and innovation labs. You'll have a clear path to promotion while working on cutting-edge technology in a dynamic and forward-thinking environment.\n",
      "\n",
      "As a Data Engineer, you will be responsible for designing, building, and managing the data pipelines, primarily within Azure and Databricks.\n",
      "\n",
      "The Role\n",
      "\n",
      "* Design, build and maintain their stream and batch data pipelines with a focus on time-series data.\n",
      "* Implement advanced analytics workflows leveraging data from different sources.\n",
      "* Experience with industrial IoT, sensor data, or real-time analytics.\n",
      "\n",
      "The Successful Candidate\n",
      "\n",
      "* Experience using Spark / Databricks / Azure\n",
      "* Strong knowledge of SQL / Python\n",
      "* Experience with IoT, Sensor Data or Real-time analytics.\n",
      "* Domain experience in Manufacturing, Mechanical Engineering or natural sciences.\n",
      "* If you have knowledge with Terraform or implementing AI workflows that would be a bonus\n",
      "* Fluent German\n",
      "\n",
      "They are looking to move fast with this hire so please apply or send your CV directly to will@aifutures.group.\n",
      "\n",
      "AI Futures \\| Filling the AI skills gap ® \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'Cloud', 'Python', 'Google Cloud Platform', 'ML', 'TensorFlow', 'Google BigQuery', 'Airflow', 'Docker', 'BI'} \n",
      "\n",
      " Tagesspiegel is one of the most-cited newspapers in Germany and it has the highest sold circulation of all newspapers in the capital region, with over 14 million online readers throughout Germany. The editorial team and publishing house are continuously expanding in order to meet the challenges of the media market with innovations. Exciting magazines and an extensive event and conference business round off the multimedia offering. The Tagesspiegel belongs to DvH Medien GmbH, owned by Dieter von Holtzbrinck, as well as Die Zeit, Handelsblatt, and WirtschaftsWoche.  \n",
      "\n",
      "The BI and Analytics team at Tagesspiegel supports the publishing house and the editorial team with meaningful reports and precise analyses - with the aim of enabling informed decisions based on data and accompanying the leading medium from the capital on its way into the digital future.  \n",
      "\n",
      "With our team - currently consisting of one engineer and three analysts - we cover the entire data value chain - from tracking in the web and app to data processing and the use of data for reports, ad-hoc analyses, CRM and data science applications.  \n",
      "\n",
      "As a Data Engineer (d/m/w), you will be an essential part of our growing team.  \n",
      "\n",
      "**These are your tasks:**\n",
      "\n",
      "* ETL processes: You develop, test and optimise data pipelines using Airflow and Python to provide a reliable basis for internal applications ranging from reporting and CRM to ML and AI applications.\n",
      "* Connection of data sources: You connect data sources to our DWH (BigQuery) and automate the data transformation.\n",
      "* DWH Management: You optimise data models and structures and enable efficient processing and provision of data.\n",
      "* Data Operations: You analyse requirements for data-related projects and find optimal solutions for implementation.\n",
      "* Data Quality \\& Consistency: You design solutions for the continuous optimisation of the data infrastructure to ensure data quality and guarantee consistent and reliable data.\n",
      "* Data availability: You ensure the stability of data processing procedures and monitor data availability in order to maximise the quality of downstream applications.   \n",
      "\n",
      "**What you bring to the table:**\n",
      "\n",
      "* Relevant background: you have completed a technical degree or relevant training.\n",
      "* ETL skills: You have experience in using Apache Airflow and Python or comparable tools.\n",
      "* Database knowledge: You are familiar with BigQuery and related Google Cloud products.\n",
      "* Software development processes: You have experience working with software development processes \\& tools (Jira, Git, Docker).\n",
      "* Communication skills: You have the ability not only to understand complex concepts, but also to communicate them clearly.\n",
      "* ML experience: Ideally, you already have some experience in implementing machine learning pipelines and experience with ML frameworks (e.g. Tensorflow or Keras).   \n",
      "\n",
      "**What we offer:**\n",
      "\n",
      "* Opportunity to work on impactful projects reaching millions of readers within an established media company\n",
      "* Collaborative environment with emphasis on professional growth\n",
      "* Fair compensation and additional benefits such as a BVG company ticket and direct insurance\n",
      "* Work at the intersection of technology and digital journalism\n",
      "* Flat structures, short decision-making processes, and direct communication\n",
      "* Creative freedom to contribute and implement your own ideas\n",
      "* Flexible working hours, 38.5 hours per week as well as 30 Vacation days (applying to a full-time role)\n",
      "* Home office  \n",
      "\n",
      "As an employer, Tagesspiegel stands for equal opportunities and respectful interaction with one another. Fair employment opportunities, regardless of ethnic or social background, gender, religion, worldview, age, sexual identity, or disability, are a matter of course for us. A valuing and motivating work environment is our common incentive and goal.  \n",
      "\n",
      "**Do you think we should get to know each other?** Then apply now through our job portal **https://verlagsjobs.tagesspiegel.de/** and send us your complete application documents including your resume, earliest possible availability, and salary expectations.  \n",
      "\n",
      "**We look forward to getting to know you!** Das BI- und Analytics-Team beim Tagesspiegel unterstützt den Verlag und die Redaktion mit aussagekräftigen Reports und präzisen Analysen - mit dem Ziel, informierte Entscheidungen auf der Basis von Daten zu ermöglichen und das Leitmedium aus der Hauptstadt auf seinem Weg in die digitale Zukunft zu begleiten.  \n",
      "\n",
      "Mit unserem Team - aktuell bestehend aus einem Engineer und drei Analysten - decken wir die gesamte Datenwertschöpfungskette ab - vom Tracking in Web und App über die Datenverarbeitung bis hin zur Verwendung der Daten für Reports, Ad-hoc-Analysen, CRM und Data-Science-Anwendungen.  \n",
      "\n",
      "Als Data Engineer:in (d/m/w) bist Du wesentlicher Bestandteil unseres wachsenden Teams.  \n",
      "\n",
      "**Das sind Deine Aufgaben:**\n",
      "\n",
      "* ETL-Prozesse: Du entwickelst, testest und optimierst Data Pipelines mit Hilfe von Airflow und Python, um eine verlässliches Basis für interne Anwendungen von Reporting und CRM bis hin zu ML- und KI-Anwendungen bereitzustellen.\n",
      "* Anbindung von Datenquellen: Du bindest Datenquellen an unser DWH (BigQuery) an und automatisierst die Datentransformation.\n",
      "* DWH Management: Du optimierst Datenmodellen und -strukturen und ermöglichst eine effiziente Verarbeitung und Bereitstellung von Daten.\n",
      "* Data Operations: Du analysierst Anforderungen datenbezogener Projekte und findest optimale Lösungen für die Umsetzung.\n",
      "* Datenqualität \\& -konsistenz: Du gestaltest Lösungen zur kontinuierlichen Optimierung der Dateninfrastruktur, um die Datenqualität zu sichern und konsistente und zuverlässige Daten zu gewährleisten.\n",
      "* Datenverfügbarkeit: Du stellst die Stabilität der Datenverarbeitungsprozesse sicher und überwachst die Datenverfügbarkeit, um so eine maximale Qualität der nachgelagerten Anwendungen sicherzustellen.   \n",
      "\n",
      "**Was bringst Du mit:**\n",
      "\n",
      "* Relevanter Hintergrund: Du hast ein abgeschlossenes technisches Studium oder eine relevante Ausbildung.\n",
      "* ETL-Kenntnisse: Du hast Erfahrung im Umgang mit Apache Airflow und Python oder vergleichbaren Tools.\n",
      "* Datenbankkenntnisse: Du bist vertraut mit BigQuery und angrenzenden Produkten der Google Cloud.\n",
      "* Softwarentwicklungsprozesse: Du hast Erfahrung im Umgang mit Softwarentwicklungsprozessen \\& -tools (Jira, Git, Docker).\n",
      "* Kommunikationsfähigkeit: Du hast du Fähigkeit, komplexe Konzepte nicht nur zu verstehen, sondern auch verständlich zu kommunizieren.\n",
      "* ML-Erfahrung: Idealerweise hast Du bereits erste Erfahrung in der Implementierung von Machine Learning Pipelines und Erfahrung mit ML-Frameworks (z.B. Tensorflow oder Keras).   \n",
      "\n",
      "**Das bieten wir Dir:**\n",
      "\n",
      "* Eine von Teamwork, Kreativität und Eigenverantwortung geprägte Tätigkeit im Herzen Berlins in einem aufstrebenden Medienunternehmen\n",
      "* Die Möglichkeit, an wirkungsvollen Projekten zu arbeiten, die Millionen von Lesern erreichen\n",
      "* Flache Strukturen, kurze Entscheidungswege, direkte Kommunikation\n",
      "* Kreativen Freiraum, um eigene Ideen einzubringen und umzusetzen\n",
      "* Faire Vergütung und zusätzliche Benefits wie z.B. BVG-Firmenticket, Direktversicherung\n",
      "* Flexible Arbeitszeiten, 38,5-Stunden-Woche sowie 30 Tage Urlaub (bei einer Anstellung in Vollzeit)\n",
      "* Mobiles Arbeiten  \n",
      "\n",
      "Als Arbeitgeber steht der Tagesspiegel für Chancengleichheit und einen respektvollen Umgang miteinander. Faire Beschäftigungsmöglichkeiten, unabhängig von ethnischer oder sozialer Herkunft, Geschlecht, Religion, Weltanschauung, Alter, sexueller Identität oder Behinderung, sind für uns selbstverständlich. Ein wertschätzendes und motivierendes Arbeitsumfeld ist unser gemeinsamer Ansporn und unser Ziel.  \n",
      "\n",
      "**Du denkst, wir sollten uns kennenlernen?** Dann bewirb Dich jetzt, über unser Stellenportal **https://verlagsjobs.tagesspiegel.de/** und sende uns Deine vollständigen Bewerbungsunterlagen inkl. Anschreiben, Lebenslauf und frühestmöglicher Verfügbarkeit.  \n",
      "\n",
      "**Wir freuen uns darauf, Dich kennenzulernen!** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'Databricks', 'SQL', 'PostgreSQL', 'Snowflake', 'DBT', 'Azure'} \n",
      "\n",
      " I am working for an established client of mine who is looking for a Senior Data Engineer with experience in MS Azure to join them on a freelance basis.\n",
      "\n",
      "The ideal candidate will have the relevant experience:\n",
      "\n",
      "1. 5 years experience in Data Architecture and Development\n",
      "2. Strong SQL experience with Database Management Systems such as PostgreSQL or Snowflake\n",
      "3. Big Data Experience with technologies such as Databricks, DBT, ADF\n",
      "4. MS Azure tech stack\n",
      "5. Great understanding of Data lakes, ETL Processes and data warehousing\n",
      "6. Knowledge of Data Modelling\n",
      "7. Strong German and English skills \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'Containers', 'Python', 'LLM', 'Amazon Web Services', 'Looker', 'Delta Lake', 'Kubernetes', 'Databricks', 'Spark', 'SQL', 'Airflow', 'DevOps', 'Tableau'} \n",
      "\n",
      " **Job description**\n",
      "\n",
      "Payla operates a cutting-edge Buy Now, Pay Later platform tailored for Payment Providers and Financial Institutions. We empower our partners, including globally recognized Payment Service Providers, to deliver seamless invoice and installment payment solutions, meeting the increasing demand for fast and frictionless transactions.\n",
      "\n",
      "On this journey, we tackle exciting challenges in areas like risk management, payment processing, and accounts receivable management --- all while prioritizing an exceptional customer experience. If you're passionate about being part of an innovative and dynamic environment and eager to collaborate on shaping the future with us, your next opportunity is here!\n",
      "\n",
      "Join our team! We're excited to welcome a **Senior Data Engineer**(all genders) to our Data Platform team.\n",
      "\n",
      "This position is -- in the spirit of Payla -- remote first.\n",
      "\n",
      "**What you will do:**\n",
      "\n",
      "* You will create, own, maintain, and develop our Deltalake Data Warehouse Platform based on Dagster and Databricks using Python, Spark and Polars to ensure its robustness, scalability, and extensibility\n",
      "* You will act as an important link between our DevOps, Data Science, Analysts, and software engineers\n",
      "* Your ETL processes will make sure that reliable data is available to our data consumers, such as Risk Analysts, Data Scientists, and other departments, as quickly and efficiently as possible\n",
      "* You will build the foundations for our Data Scientists to bring our Machine Learning models to the next level\n",
      "* You will enjoy making your code as efficient and readable as possible\n",
      "* You will take ownership of the quality of data being processed\n",
      "* You will contribute your own ideas to make the flow of our data even better\n",
      "\n",
      "**Job requirements**\n",
      "\n",
      "**What you will bring to the team:**\n",
      "\n",
      "* You have a bachelor's degree in computer science or a related field (or good arguments, why this was a waste of time)\n",
      "* You have at least two years of experience as a Senior Data engineer or in a similar role\n",
      "* You are comfortable working with Python, Containers, and CI infrastructure\n",
      "* You have working experience with scheduling tools such as Dagster or Apache Airflow\n",
      "* You have experience with data modeling, ETLs, data warehousing and SQL\n",
      "* You have already crafted data solutions and deployed data pipelines in production\n",
      "* Experience with building reports and using reporting tools (e.g. Databricks, Apache Superset, Looker, Tableau) is a plus\n",
      "* Having knowledge in AWS, Kubernetes, and DevOps principles is a plus\n",
      "* You are fluent in both written and spoken English, German is a plus\n",
      "* You are a team player with a caring personality\n",
      "* For us it is most important that you bring in the right engineering mindset and that you are curious on tackling upcoming architectural challenges while being able to solve the details on your own or in consultation with our Software Engineers\n",
      "\n",
      "**What Payla offers:**\n",
      "\n",
      "* Team: An encouraging, passionate and supportive team environment\n",
      "* Flexibility: We will create a professional work environment for you at your remote location\n",
      "* Vacation: 30 days paid per year\n",
      "* Start-up Spirit: Exciting challenges from Day 1 with focus on results, mixed with less hierarchy and less meetings (as well as less bullshit-bingo) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'Python', 'ML', 'SQL', 'Deep learning', 'PostgreSQL', 'Azure'} \n",
      "\n",
      " What we are doing\n",
      "\n",
      "Our mission is to prevent vision loss and ultimately blindness by developing AI software that assists eye doctors in therapy planning for their patients. We use computer vision deep learning models(AI) trained on thousands of cases and millions of images to identify and predict disease progression. The decision support algorithm targets common eye diseases like Age-related Macular Degeneration (AMD, 7.5mio affected in Germany alone).\n",
      "\n",
      "Tasks\n",
      "- ----\n",
      "\n",
      "Your responsibilities\n",
      "\n",
      "* You are responsible for building and optimizing data pipelines to bring together information from different source systems, i.e. imaging and clinical data\n",
      "* You prepare data for analytical or operational uses, i.e. by integration, consolidation, cleansing, and structuring of data\n",
      "* You bring curiosity, problem-solving instinct and statistical and mathematical knowledge.\n",
      "\n",
      "Requirements\n",
      "- -----------\n",
      "\n",
      "What we are looking for\n",
      "\n",
      "Your must-haves\n",
      "\n",
      "* Experience in data engineering related to computer vision and imaging analysis (2D/3D image processing, segmentation, image normalization, ideally medical imaging)\n",
      "* Applied knowledge of deep and machine learning (ML) techniques\n",
      "* Experience in programming in Python, in developing software in AI and data-centric domains\n",
      "\n",
      "Your nice-to-haves\n",
      "\n",
      "* Studies in data science/engineering, bioengineering, computer science, medical physics or similar\n",
      "* Experience in working with relational databases (e.g. PostgreSQL), and modern, scalable storage technologies (e.g. Azure FileShare)\n",
      "* Working-level English (C1 or higher)\n",
      "\n",
      "Benefits\n",
      "- -------\n",
      "\n",
      "What we offer\n",
      "\n",
      "* Direct impact on improving quality of life for patients \\& doctors\n",
      "* High personal responsibility for your project \\& fast personal growth\n",
      "* Flexible working hours, remote work \\& office at Munich main station\n",
      "* Startup culture \\& an international rock star team\n",
      "\n",
      "Reach out to us with information about your availability, and a link to your conclusive profile (GitHub, LinkedIn, CV). Please share with us why you want to help us in preventing blindness with AI.\n",
      "\n",
      "At deepeye Medical, we take data protection and privacy seriously: https://drive.google.com/file/d/1zMccoHiH6Wg1kU2e-FRhbAO71B6XkG8R/view?usp=sharing \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'Python', 'Databricks', 'Spark', 'SQL', 'Microsoft Excel', 'PySpark'} \n",
      "\n",
      " About this job\n",
      "- -------------\n",
      "\n",
      "The DeepImmo Data \\& Analytics team contributes the quantitative edge into our product by building the data groundwork and bringing the data alive. To that end, we integrate an expansive set of data sources and prepare machine learning models as well as prototypes of analytical methods for their implementation in our Real Estate OS. Your contribution within this role will see you focusing on building out the most important of these components: The fundamental data landscape.\n",
      "\n",
      "Your Role\n",
      "- --------\n",
      "\n",
      "* Work closely together with our product and engineering teams to provide high-value data for our customers. Your datasets will be a core part of our customer-facing Real Estate OS. This means what you build will see lots of use, but with that also have a high demand for quality.\n",
      "* Build PySpark data-pipelines in Databricks to transform our raw data into well-modeled and quality-assured data that fuels innovative insights.\n",
      "* Implement data retrieval using APIs and Web Scraping from different sources relevant to location, market, and price of real estate to expand our data treasury and provide a thorough foundation for upstream processing.\n",
      "* Keep the data flowing: You will observe and improve our data pipelines and intervene if things go wrong.\n",
      "* You keep a wholesome perspective on our tool- and infrastructure choices and closely participate in decisions that shape our stack going forward\n",
      "\n",
      "Your Profile\n",
      "- -----------\n",
      "\n",
      "* You excel at self-organization and embrace taking full ownership of your work\n",
      "* You have a Master's Degree (or Bachelor's with comparable work experience) in Computer Science, Mathematics, Data Science, or a related field\n",
      "* You have collected 2-3 years of working experience in the broader data area\n",
      "* You're very well-versed with SQL, Python and PySpark\n",
      "* You bring experience in working with Databricks or are motivated to learn\n",
      "* Know-How in TypeScript \\& NodeJS are optional\n",
      "* Experience in the real estate industry is a plus, a strong willingness to dive deep into real estate data is expected.\n",
      "* Fluency in English is a must-have, good German skills are beneficial \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'Cloud', 'Python', 'Amazon Web Services', 'Java', 'SQL', 'Azure'} \n",
      "\n",
      " **About The Company** GROPYUS creates sustainable, affordable, and aspirational buildings for everyone through modular construction and setting a new standard in smart living.  \n",
      "\n",
      "**About The Role** We are growing our Data Language Team within our Digital Twin department, which by creating a semantic Digital Twin will enable hyper-automation of buildings as products. By creating a Gropyus-wide language for data using semantic modelling we make data interoperable across domains, and render it machine readable.  \n",
      "\n",
      "The Data Language team interacts with experts from various domains such as Sustainability, AI, IoT, smart factory, construction engineers, building architects, logistics experts, software engineering, and solve complex challenges pertaining to end-to-end interoperability of data throughout the lifecycle of a building and all its associated processes.  \n",
      "\n",
      "**As part of our Data Language team you will:**\n",
      "\n",
      "* Be involved in designing data- and information-models by formalizing concepts from various domains using state of the art approaches.\n",
      "* Integrate data processing pipelines for IoT, smart Factory and Building Systems.\n",
      "* Interoperate data ingestion with knowledge graph models. Implement data governance and security requirements.\n",
      "* Work with our Data Platform Team to integrate data pipelines with existing systems and applications.\n",
      "* Contribute to best practices and rigor in software development including data governance, testing, and validation   \n",
      "\n",
      "**About You**\n",
      "\n",
      "* You are experienced with at least one of the following languages: Python, Scala, Java, or generally JVM languages.\n",
      "* You have strong problem-solving and analytical skills and the ability to break down complex tasks for troubleshooting and operations\n",
      "* You have experience identifying and resolving issues related to data discrepancies and inconsistencies and creating validation and testing for prevention and handling\n",
      "* You have experience in building, operating, and scaling data intensive reactive processing pipelines   \n",
      "\n",
      "**Optional Experience:**\n",
      "\n",
      "* Knowledge or experience utilizing Graph Databases and Non-SQL query languages\n",
      "* Experience in setting up IoT information systems\n",
      "* Experience working with construction industry data and / or complex and large datasets\n",
      "* Experience with ontology engineering, and knowledge engineering topics, including using ontology engineering tools such as Protege\n",
      "* Experience with Azure (data solutions) and FaaS / Serverless or knowledge of these concepts from other cloud providers (GCP or AWS) is a strong plus   \n",
      "\n",
      "What we offer:  \n",
      "\n",
      "* An unlimited contract with 28 vacation days.\n",
      "* Flexible, hybrid work: You can work from home or come into the office on the schedule that works best for you. Feel like traveling and working from another location? No problem. Tune into meetings and work from abroad for up to 40 days per year. In addition, flex hours allow you to prioritize important moments in your day.\n",
      "* Attractive perks and benefits: We offer a company pension scheme, language courses (English/German), and employee discounts with selected sustainable brands. Furthermore, our external partners in mental health support you in strengthening your mental health through anonymous private/group sessions.\n",
      "* Virtual Shares Program: Participate in the success of GROPYUS through our Virtual Share Program.\n",
      "* Ownership: You can truly make your mark with your work and contributions -- on all our organizational levels. Find your solutions, drive and test them.\n",
      "* The chance to be part of something big: Our mission is to rethink an entire industry. Join us in reinventing construction and fostering sustainable, affordable living.\n",
      "* A diverse and enriching work environment: Become a valued member of a wonderfully diverse team of individuals from over 50 countries. At GROPYUS we value inclusion above all, ensuring that everyone feels they belong -- no matter their background. In this way, our differences become strengths that help us grow as a team.   \n",
      "\n",
      "Join us on our mission to design buildings as continuously evolving products to create the most exciting and affordable experience for all. We build for people and conserve the resources of our planet.  \n",
      "\n",
      "We can't wait to get to know you.  \n",
      "\n",
      "For more information, visit our website, and if you have any questions, please reach out to us via email. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'Cloud', 'Python', 'Google Cloud Platform', 'ML', 'Amazon Web Services', 'Pandas', 'Microsoft Excel', 'Azure'} \n",
      "\n",
      " **Job Title: Senior Data Engineer**   \n",
      "\n",
      "**Location:** Berlin, Germany (Hybrid Work)  \n",
      "\n",
      "**Type:** Full-Time  \n",
      "\n",
      "**Entry Date:** Immediate\n",
      "\n",
      "**Company Description:**   \n",
      "Talonic is a dynamic and innovative AI start-up dedicated to making data and analytics accessible to everyone. We are looking for a passionate and experienced Data Engineer to join our growing team. As Data Engineer, you will play a pivotal role in shaping our data infrastructure and engineering practices. Talonic is backed by the Humboldt University of Berlin, K.I.E.Z., Science \\& Startups, NVidia Inception, Microsoft and more.\n",
      "\n",
      "This is a fantastic opportunity for someone who wants to join a small, fast-paced team and strengthen their experience in AI, explore modern data engineering, and shape rapid growth from the very beginning.\n",
      "\n",
      "**Job Description:**   \n",
      "As a Data Engineer at Talonic, you will be hands-on in designing, building, and optimizing data systems that support our AI and analytics products. This role is perfect for someone with a high level of ownership and a desire to make meaningful contributions to a growing company.\n",
      "\n",
      "At Talonic, we value learning and innovation. You will have all the support and guidance you need but will also be encouraged to experiment, explore, and find solutions that align with our vision.\n",
      "\n",
      "**Key Responsibilities:**\n",
      "\n",
      "* Design, develop, and maintain robust data pipelines and ETL processes to handle large volumes of structured and unstructured data.\n",
      "* Collaborate with our Data Scientist, CTO and software engineers to build scalable solutions for ETL, machine learning models and AI functions.\n",
      "* Optimize and manage data storage solutions, including experience with vector databases and retrieval mechanisms.\n",
      "* Develop and maintain APIs and integrations for data access and retrieval to support various applications.\n",
      "* Implement data quality checks, validation, and monitoring to ensure high data integrity and availability.\n",
      "* Work closely with product development to ensure that data solutions align with overall company goals and strategies.\n",
      "* Continuously evaluate and integrate new data engineering tools, libraries, and technologies that can enhance our data infrastructure.\n",
      "* Shape and define data engineering best practices, making a lasting impact on our development processes and culture.\n",
      "* Ensure robust security measures and data privacy standards are in place and compliant with regulations.\n",
      "\n",
      "**Qualifications:**\n",
      "\n",
      "* Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.\n",
      "* 3-5 years of experience as a Data Engineer or in a similar role, preferably within AI or tech start-ups. We are open to this being your first senior position if you can demonstrate your experience and determination.\n",
      "* Strong proficiency in Python, with extensive experience working with data manipulation libraries such as Pandas.\n",
      "* Experience with ML and AI tools and frameworks.\n",
      "* Proficiency in working with vector databases and retrieval mechanisms, optimizing them for AI applications.\n",
      "* Great understanding of ETL processes, data pipelines, and data architecture design.\n",
      "* Experience with cloud platforms such as AWS, Azure, or Google Cloud, and their data services.\n",
      "* Familiarity with Pandas AI is a plus.\n",
      "* High agency, self-motivation, and a hands-on approach to problem-solving.\n",
      "* Excellent communication skills and the ability to work in a fast-paced, collaborative environment.\n",
      "* Fluent in English.\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "* **Salary:** Based on experience, with the opportunity for a significant increase after our next fundraising round.\n",
      "* **Equity Options:** Expect a generous equity program, ensuring that you get a stake in the company that makes our overall success also your personal success.\n",
      "* **Remote Work:** 100% remote position, with an option to use our office at Humboldt University or at K.I.E.Z. in Berlin. We occasionally participate in start-up or industry events that require in-person attendance.\n",
      "* **Location:**The hire needs to be located in Berlin or within close, physical proximity.\n",
      "* **Growth Opportunity:** Gain deep insights into all aspects of the business and take your career to the next level. Be a part of a fast-growing start-up where you can make a real impact. We are open to this being your first senior position and to involve you in each future data engineering hire, potentially building you to a managerial function if desired. We are also open to you nerding out only by yourself on data for the forseeable future.\n",
      "* **Supportive Culture:** We promote a safe, inclusive, and supportive work environment where everyone's ideas are valued, and we avoid micro-management.\n",
      "* **Learning Environment:** We believe in continuous learning and growth, providing a space for you to explore new technologies, methodologies, and approaches in data engineering.\n",
      "\n",
      "**How to Apply:**   \n",
      "Please send your resume and a cover letter explaining why you are a good fit for this role to [\\[email protected\\]](/cdn-cgi/l/email-protection). Include \"Senior Data Engineer in Berlin\" in the subject line.\n",
      "\n",
      "Talonic is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.\n",
      "\n",
      "Job Type: Full-time\n",
      "\n",
      "Ausbildung:\n",
      "\n",
      "* Bachelor (Erforderlich)\n",
      "\n",
      "Sprache:\n",
      "\n",
      "* Englisch (Erforderlich)\n",
      "\n",
      "Work Location: In person\n",
      "\n",
      "Art der Stelle: Vollzeit\n",
      "\n",
      "Gehalt: 39.449,27€ - 112.577,20€ pro Jahr\n",
      "\n",
      "Leistungen:\n",
      "\n",
      "* Homeoffice-Möglichkeit\n",
      "\n",
      "Möglichkeit zu pendeln/umzuziehen:\n",
      "\n",
      "* 13355 Berlin: bereit sein zu pendeln oder vor Antritt der Stelle einen Umzug zu planen (Wünschenswert)\n",
      "\n",
      "Arbeitsort: Zum Teil im Homeoffice in 13355 Berlin \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'Cloud', 'Python', 'Amazon Web Services', 'Amazon Glue', 'Amazon S3', 'DevOps'} \n",
      "\n",
      " * Develop and optimize data pipelines using AWS Glue and Lambda functions.\n",
      "* Utilize Python to build reusable modules, APIs (Fast API/Flask/Django), and automation scripts.\n",
      "* Lead project teams in designing end-to-end data pipelines and ETL workflows.\n",
      "* Troubleshoot data processing issues and optimize performance for ETL workflows. Experiences: Experienced in working with Python projects for Data Processing, Data Transformation, and building reusable and modular components. Experience in Data Migration is a plus.\n",
      "* Experienced in both classical Python development as well as in Python Notebooks.\n",
      "* Experienced in AWS components such as S3, Glue, Lambda, and Kinesis (KDS) - Experienced with DevOps and Infrastructure oversight.\n",
      "* 5-8 years of experience in Data Engineering with a focus on cloud-based solutions.\n",
      "* Strong expertise in AWS services like S3, Glue, Lambda, and IAM configurations.\n",
      "* Bachelor's degree in Computer Science, Information Technology, or related fields. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'Python', 'Microsoft Excel', 'Scikit-learn', 'Agile', 'SQL', 'JavaScript', 'DevOps', 'NumPy'} \n",
      "\n",
      " Are you passionate about combining your expertise in data engineering with cutting-edge technologies in telecommunication? Do you have a strong background in scripting (Python, JavaScript) and experience with mobile protocols (LTE/4G, 5G, L1, RLC, RRC, NAS)? We're looking for an experienced Data Engineer to join our team and help develop next-generation products for a global leader in consumer electronics. You'll play a key role in developing frameworks and test environments, analyzing and processing mobile protocol and firmware data for devices sold in millions of units worldwide.\n",
      "\n",
      "As part of a top US tech company, you'll be part of an agile, cross-functional team, working directly with customer teams and using your expertise to drive impactful solutions. If you're a proactive, hands-on engineer with excellent communication skills and a passion for tackling complex problems, we want to hear from you!\n",
      "\n",
      "**Key Responsibilities:**\n",
      "\n",
      "**Data Engineering \\& LTE Protocol Analysis:**\n",
      "\n",
      "* **Analyze and process mobile protocol data (LTE/4G, 5G, L1, RLC, RRC, NAS) and firmware data for consumer electronics devices.**\n",
      "* Develop and maintain Python- and JavaScript-based tools and scripts to process and analyze large volumes of mobile data.\n",
      "* Translate mobile protocol KPIs into working scripts to measure and improve protocol performance.\n",
      "* Collaborate with cross-functional teams to troubleshoot and optimize mobile features.\n",
      "\n",
      "**Requirements:**\n",
      "\n",
      "* University degree in Computer Science, Telecommunications, Electrical Engineering, or similar.\n",
      "* 5 years of experience as a Data Engineer, Software Engineer, Software Tester, or DevOps utilizing Python and JavaScript\n",
      "* Solid in handling Linux and able to adjust to changing work environments and systems\n",
      "* Strong hands-on experience with Python, incl. data structures, OOP, file handling, exception handling, multi-threading, statistical libraries (e.g., Numpy, Scipy, Scikit-learn)\n",
      "* Solid knowledge of LTE technology, including control layers (L1, RLC, RRC, NAS), as well as data plane protocols.\n",
      "* Strong experience working in agile environments, with proactive and collaborative communication skills.\n",
      "\n",
      "**Nice to Have:**\n",
      "\n",
      "* Experience with 3G and 5G technologies (NSA \\& SA).\n",
      "* Familiarity with web frameworks such as Django or Flask.\n",
      "* Experience with ORM (Object Relationship Mapping) libraries.\n",
      "* Experience with Python GUI programming (e.g., PyQt, TkInter).\n",
      "* Understanding of multi-process architectures.\n",
      "* Experience with SQL databases (e.g., SQLite).\n",
      "* Understanding of applying Data Science and Machine Learning paradigms in Python.\n",
      "\n",
      "Job Types: Full-time, Permanent\n",
      "\n",
      "Application Question(s):\n",
      "\n",
      "* Do you have a valid VISA and work permit for Germany (only if applicable)?\n",
      "\n",
      "Experience:\n",
      "\n",
      "* Python: 4 years (Required)\n",
      "* Telecommunication: 3 years (Required) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'Cloud', 'Agile', 'SAP'} \n",
      "\n",
      " * Data Engineering Senior Developer - SAP BTP, FIORI, UI5 (m/w/d)  \n",
      "\n",
      "**Technology** -- SAP Fiori, UI5  \n",
      "\n",
      "**Why Infosys**   \n",
      "\n",
      "We invite you to be part of and be inspired by an open-minded way of working in our multinational corporation! If you want to accompany us on the way to the future of IT, you prefer innovation instead of boring repetitive project work and you want to experience growth instead of stagnation, please apply now!  \n",
      "\n",
      "We offer exciting challenges in one of the most dynamically growing SAP consulting companies in Germany and Europe. With us you will further expand your experience and skills within a global corporate culture together with qualified and motivated colleagues. Our growth plans open great career opportunities in a highly stimulating environment. With this growth you will get a chance to lead large global teams who work in the digital transformation journeys of our customers, mostly the DAX 30 or Fortune 1000 organizations.  \n",
      "\n",
      "**Job Description**\n",
      "\n",
      "* Design and development of customized cloud applications using SAP Fiori, SAP UI5 and other relevant technologies.\n",
      "* Implementation automation solutions such as SAP Intelligent RPA for the automation of business processes.\n",
      "* Collaboration with the integration team to connect and integrate SAP BTP with other systems and applications in the company.\n",
      "* Recording and analyzing requirements from specialist departments to develop user friendly and functional apps that meet business needs.\n",
      "* Ensuring the scalability, performance and user friendliness of the developed applications through efficient UX/UI design.\n",
      "\n",
      "**Required**\n",
      "\n",
      "* Sound knowledge in the development of cloud applications on SAP BTP, in particular with SAP Fiori, SAP UI5 and other relevant web technologies.\n",
      "* Experience in the implementation of automation solutions, including SAP Intelligent RPA.\n",
      "* Knowledge of system and data integration, in particular the connection of SAP BTP to other company systems.\n",
      "* Strong analytical skills for recording and processing requirements from specialist departments and for developing user-oriented apps.\n",
      "* Expertise in optimizing app performance and user experience, with a focus on scalability, performance and user-friendly UX/UI design.\n",
      "* Require bilingual capability in German and English.\n",
      "* Open to local travel to client premises in Munich and Frankfurt.\n",
      "\n",
      "**Preferred**\n",
      "\n",
      "* Good communication skills\n",
      "* Quick comprehension\n",
      "* Working in agile teams\n",
      "* Good team player\n",
      "\n",
      "**Overview**   \n",
      "Infosys is a global leader in next-generation digital services and consulting. We enable clients in more than 50 countries to navigate their digital transformation.   \n",
      "With over four decades of experience in managing the systems and workings of global enterprises, we expertly steer our clients through their digital journey. We do it by enabling the enterprise with an AI-powered core that helps prioritize the execution of change. We also empower the business with agile digital at scale to deliver unprecedented levels of performance and customer delight. Our always-on learning agenda drives their continuous improvement through building and transferring digital skills, expertise, and ideas from our innovation ecosystem.   \n",
      "\n",
      "**All aspects of employment at Infosys are based on merit, competence and performance. We are committed to embracing diversity and creating an inclusive environment for all employees. Infosys is proud to be an equal opportunity employer.** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'Cloud', 'Amazon Redshift', 'Python', 'Google Cloud Platform', 'MongoDB', 'Amazon Web Services', 'Java', 'MySQL', 'NoSQL', 'Google BigQuery', 'SQL', 'Airflow', 'PostgreSQL', 'BI', 'Microsoft Excel', 'PowerBI', 'Azure'} \n",
      "\n",
      " **At Bayer we're visionaries, driven to solve the world's toughest challenges and striving for a world where ,Health for all, Hunger for none' is no longer a dream, but a real possibility. We're doing it with energy, curiosity and sheer dedication, always learning from unique perspectives of those around us, expanding our thinking, growing our capabilities and redefining 'impossible'. There are so many reasons to join us. If you're hungry to build a varied and meaningful career in a community of brilliant and diverse minds to make a real difference, there's only one choice.** **Data Engineer Consumer Health Cluster Germany/Austria (all genders)**   \n",
      "\n",
      "**Your Tasks And Responsibilities**\n",
      "\n",
      "* You design, develop, and maintain robust data pipelines as part of your role, facilitating the collection, transformation, and storage of data from various internal and external sources\n",
      "* Creating a robust data model for flexible data analysis in our analysis platform is your responsibility\n",
      "* You collaborate with cross-functional teams to understand data requirements and ensure that the data architecture aligns with business goals\n",
      "* Monitoring and optimizing data systems for performance, reliability, and scalability is part of your role, ensuring high data availability\n",
      "* You implement data quality checks and validation processes to maintain data integrity and accuracy\n",
      "* Developing and maintaining documentation for data models, data flows, and ETL processes, as well as general data governance initiatives, is your responsibility to ensure clarity and knowledge sharing\n",
      "* Troubleshooting and resolving data-related issues is part of your role, providing support to users and stakeholders as needed\n",
      "* Staying updated on emerging technologies and best practices in data engineering is key to continuously enhancing data processes and systems in your position   \n",
      "\n",
      "**Who You Are**\n",
      "\n",
      "* You hold a Bachelor's degree in Computer Science, Information Technology, Data Science, or a related field. Alternatively, you have several years of relevant experience with ongoing education in the field. A Master's degree is a plus\n",
      "* You have several years of experience in data engineering or a related field, with a strong focus on data pipeline development, data warehousing, and data management\n",
      "* Experience working within the OTC or FMCG environment is a plus\n",
      "* You are proficient in data modeling, programming (e.g., Python, Java, Scala), SQL, and have experience with both relational (e.g., PostgreSQL, MySQL) and NoSQL (e.g., MongoDB, Cassandra) databases\n",
      "* You are experienced with data warehousing (e.g., AWS Redshift, Google BigQuery), ETL tools (e.g., Apache Airflow, Talend), and cloud platforms (e.g., AWS, Azure, Google Cloud), and have experience designing multidimensional models (e.g., PowerBI, MS Analysis Services)\n",
      "* You have knowledge of data analytics concepts, statistical analysis, and are skilled in data visualization using tools like Power BI to create impactful reports and dashboards\n",
      "* You possess strong analytical and problem-solving skills, capable of analyzing complex data sets and collaborating with data analysts to translate business needs into technical solutions\n",
      "* You have excellent communication skills in both German and English, with the ability to collaborate effectively with both technical and non-technical stakeholders   \n",
      "\n",
      "**What We Offer** **What matters to you, matters to us!**\n",
      "\n",
      "* We ensure your financial stability through a competitive compensation package, consisting of an attractive base pay and our annual bonus. In addition, managers can recognize special contributions by granting an individual performance award.\n",
      "* Whether it's hybrid work models or part-time arrangements: Whenever it is possible, you will have the flexibility to work how, when and where it is best for you.\n",
      "* Your family is a top priority. We offer loving company daycare centers at multiple locations, support in finding childcare, time off for the care of elderly or dependent family members, summer camps for children, and much more.\n",
      "* We support your professional growth by providing access to learning and development opportunities, training programs through the Bayer Learning Academy, development dialogues, as well as coaching and mentoring programs.\n",
      "* We promote health awareness and opportunities for selfcare through various measures, such as free health checks with the company doctor and our health platform #machfit.\n",
      "* We embrace diversity by providing an inclusive work environment in which you are welcomed, supported, and encouraged to bring your whole self to work.   \n",
      "\n",
      "**The position is to be filled at Bayer Vital GmbH.** **YOUR APPLICATION**   \n",
      "\n",
      "**Location:**   \n",
      "\n",
      "**Division:**   \n",
      "\n",
      "**Reference Code:** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'Cloud', 'Databricks', 'Lakehouse', 'SQL', 'Airflow', 'DBT', 'PowerBI', 'Azure'} \n",
      "\n",
      " As Golding sets its sights on growth, we are seeking a **(Senior)** **Data Engineer (m/f/d)** to bolster our committed Operations Team at our Munich location at the earliest opportunity.\n",
      "\n",
      "**Your responsibility**\n",
      "\n",
      "Golding is currently navigating through a transformative phase, with a vision to evolve into a data-driven enterprise. As a Data Engineer, you play a pivotal role in shaping this digital transformation and leveraging data as a valuable corporate asset. Your responsibility spans the creation of a novel data and analytics platform, contributing significantly to the execution of our data and analytics strategy. Your expertise is instrumental in paving the way towards a data-centric organizational landscape.\n",
      "\n",
      "**This includes:**\n",
      "\n",
      "* Driving forward the creation of a new company-wide data \\& analytics platform based on a lakehouse.\n",
      "* Analysis and documentation of technical requirements.\n",
      "* Design, develop and maintain modern data structures within the lakehouse framework (e.g. Azure).\n",
      "* Conception, development and management data flows.\n",
      "* Development and maintainance of data outputs such as dashboards and reports.\n",
      "* Provide Support and maintainance of the existing Data Warehouse (DWH) environment up to its decomission.\n",
      "\n",
      "**Your qualification**\n",
      "\n",
      "* A STEM degree or a comparable qualification in a related field.\n",
      "* Minimum 5 years of experience in a professional IT environment.\n",
      "* Demonstrable experience in project management, particularly in leading IT projects.\n",
      "* Extensive expertise in building high-performance data management processes.\n",
      "* Proficiency in relational databases and query languages like SQL.\n",
      "* Strong understanding of data modeling principles.\n",
      "* Practical experience with tools such as Databricks, DBT, Airflow, PowerBI, or similar platforms.\n",
      "* Proven track record in (sub-)project management for implementing a DWH/Analytical platform.\n",
      "* Familiarity with cloud technologies like Microsoft Azure or similar environments would be advantageous.\n",
      "\n",
      "**What we offer you**\n",
      "\n",
      "* An attractive salary package with a performance-based component and employer-sponsored retirement savings plan.\n",
      "* Opportunities for training and certifications in current technologies and frameworks.\n",
      "* The benefits of a modern office in the charming east of Munich, along with the flexible option to work remotely.\n",
      "* Clear pathways for career advancement and leadership roles.\n",
      "* Work on challenging and impactful projects that make a difference.\n",
      "* Intensive onboarding and a buddy program for a smooth start.\n",
      "* Dynamic work environment fostered by open communication and flat hierarchies.\n",
      "* The opportunity to be part of an international expert team where cohesion and collective connection are prioritized, helping you further develop your skills and find innovative solutions.\n",
      "* Regular team events and celebrations for a vibrant, communal atmosphere.\n",
      "* Discounts on meals at the company restaurant and various additional perks, including support for a job bike, access to WellHub, free fruits, beverages, and much more.\n",
      "\n",
      "**About Golding Capital Partners**\n",
      "\n",
      "With an expert team of more than 200 people and assets under management of approximately €15 billion, Golding Capital Partners is one of the most respected specialists in alternative investments. Our customised investment strategies - with an increasing focus on sustainable investments - ensure the high quality and diversification of our investments.\n",
      "\n",
      "We have established ourselves as a trusted partner for institutional investors such as insurance companies, pension funds, foundations, family offices, church organisations and banks who are looking for attractive long-term investments in the areas of private equity, private credit, infrastructure, secondaries and impact and who share our commitment to sustainability and social responsibility.\n",
      "\n",
      "Our values unite the people at Golding. We foster a community in which everyone feels a sense of belonging and value diversity of perspective. The multifaceted working environment at Golding is highly recognized: awarded three times in a row since 2022 by kununu with the \"Top Company Award\" and in 2024 by ZEIT as \"Most Wanted Employer\".\n",
      "\n",
      "If you can identify with our corporate philosophy and profile, seize the opportunity and let us keep writing the success story of Golding Capital Partners together!\n",
      "\n",
      "We are looking forward to your online application!\n",
      "\n",
      "Your contact person for this position is Magdalena Dreisow (Talent Attraction \\& Branding). \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'Cloud', 'Python', 'LLM', 'Streaming', 'Grafana', 'Kubernetes', 'Orchestration', 'Batch processing', 'Apache Kafka', 'Google BigQuery', 'SQL', 'Airflow', 'PostgreSQL', 'DevOps'} \n",
      "\n",
      " **Distribusion** is the world's leading ground transportation marketplace and gives travelers seamless access to ground transportation online, from search to ticket purchase. We have built a cutting-edge B2B technology platform that connects bus, rail, and ferry operators in 70 countries with the biggest online retailers, including Google Maps and Booking.com.  \n",
      "\n",
      "We are shaping the future of travel and building the largest global network of transport providers and retailers. Having grown 10x in the past year, we are one of the fastest-growing startups in travel. Backed by four leading VCs (TQ Ventures, Creandum, Northzone, and Lightrock), and now, following our recent $80 m Series C, we are ready to push beyond.  \n",
      "\n",
      "We are looking for a **Senior** **Data Engineer** who is excited to work on complex data challenges in a rapidly evolving environment. In this role, you will manage large, high-velocity datasets and contribute to innovative solutions that are pioneering the ground transportation industry. Your work will have a tangible impact, with results visible to millions of users within hours or days.  \n",
      "\n",
      "As part of our team, you'll play a key role in building and scaling our data architecture to support the company's fast-paced growth. By integrating new data sources and maintaining existing ones, you will enable data-driven decisions that empower both internal teams and external users, driving the next phase of our data platform's evolution.  \n",
      "\n",
      "**What you will do:**\n",
      "\n",
      "* Data integration -- introducing new batch and streaming data sources, maintaining existing ones, ensuring smooth ingestion and high data quality\n",
      "* Identify opportunities to enhance the stability, reliability, and maintainability of the Data Platform, introducing new tools and impactful improvements, upgrading data architecture.\n",
      "* Collaborate with cross-functional teams to ensure data-driven insights are easily accessible and actionable for both internal and external stakeholders.\n",
      "* Become an expert in our data platform -- building \\& maintaining pipelines to feed LLM with metadata \\& context to provide data platform documentation.  \n",
      "\n",
      "**Workplace:** We are mainly looking for this role to be based within Germany, preferably in our HQ in Berlin. However, we are a remote-first company, with teams located around the Globe and open for talent based elsewhere.  \n",
      "\n",
      "**Who you are:**\n",
      "\n",
      "* Hands-on with cloud platforms, especially GCP, and strong background with Big Data tools such as BigQuery, ElasticSearch, Kafka, and Airflow.\n",
      "* Proficient in Python and SQL, with experience in frameworks like FastAPI.\n",
      "* Have a strong understanding of databases like PostgreSQL and Redis, and monitoring tools such as Prometheus and Grafana, alongside Kubernetes for container orchestration.\n",
      "* Knowledgeable about unit, integration, and end-to-end testing practices, focusing more on implementation than specific tools.\n",
      "* Comfortable with CI/CD pipelines using GitLab or GitHub and bring basic DevOps skills.\n",
      "* Experienced in using project management tools like Jira, with the ability to estimate timelines and provide reliable ETAs.\n",
      "* Passionate about tackling complex data challenges, collaborating with others, and continuously learning and sharing ideas.\n",
      "* You are driven, ambitious, and willing to get hands-on in shaping the future of ground transportation travel.\n",
      "* You show ownership and responsibility for your problem space.\n",
      "* You are fluent in English, both verbal and written.  \n",
      "\n",
      "**What you can expect:**\n",
      "\n",
      "* You will join a fast-paced travel tech company and take on a rapidly growing industry.\n",
      "* You will take on ownership and responsibility from day one and have a direct impact on the success of the company.\n",
      "* We value agility! We're constantly updating our tech stack and offer the best possible tools to ensure all of our in-house engineers, partners, and carriers benefit from cutting-edge, efficient solutions.\n",
      "* You will work with global companies. Our product attracts the biggest names in travel technology, such as Booking.com and Google Maps, and national carriers like Amtrak, Deutsche Bahn, Renfe, and SNCF.\n",
      "* You will join an international team of talented and driven people with a clear mission. Expect your colleagues to inspire, support and challenge you every day!\n",
      "* We offer flexible and remote working conditions, relocation opportunities and career growth in a small and developing company.  \n",
      "\n",
      "**Our hiring** **team for this role:** Apply to the job to enter the recruitment process and for any queries, please reach out to us at (talent@distribusion.com)  \n",
      "\n",
      "Do you want to work on a product that is used by millions of people daily with a high load, availability and scalability, and most advanced technology? Come join us! \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "position\n",
       "Data Scientist                           877\n",
       "Data Analyst                             779\n",
       "Data Engineer                            592\n",
       "Data Protection/Governance Specialist    139\n",
       "Architect                                134\n",
       "Consultant/Advisor                       100\n",
       "Software Engineer                         82\n",
       "Data Manager                              65\n",
       "Data Entry Specialist                     54\n",
       "IaC Specialist                            50\n",
       "Project Manager                           47\n",
       "Product Manager                           46\n",
       "Data Quality Specialist                   36\n",
       "Facility Engineer                         30\n",
       "Product Owner                             24\n",
       "ML Ops                                     9\n",
       "Tutor/Teacher                              2\n",
       "Network Engineer                           1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    print(x[1].skills, \"\\n\" * 2, x[1].description, \"\\n\" * 5)\n",
    "    for x in (\n",
    "        df_posting[df_posting.position == \"Data Engineer\"][[\"skills\", \"description\"]]\n",
    "        .sample(20)\n",
    "        .iterrows()\n",
    "    )\n",
    "]\n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "4b17e283-56b5-43da-bd1b-2c0c58277d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set((1, 2)) & set((2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "78eaa15c-5d41-42cb-9f6a-9a883309d5ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cloud, skills_set in cloud_skills_sets.items():\n",
    "    df_posting[\"skills\"] = df_posting[\"skills\"].map(\n",
    "        lambda x: x | {cloud} if x & skills_set else x\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "bbe48989-2d8d-49e5-a4b5-7290e1d529ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skills = (\n",
    "    df_posting[df_posting[\"position\"] == \"Data Engineer\"][[\"id\", \"position\", \"skills\"]]\n",
    "    .explode(\"skills\")\n",
    "    .dropna()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "6375799d-7baf-4251-92fd-c141a3ff4b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5359 entries, 5 to 2546\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        5359 non-null   object\n",
      " 1   position  5359 non-null   object\n",
      " 2   skills    5359 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 167.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_skills.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "c8e15e41-dfba-4807-808c-7298baa78ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "skills\n",
       "Python                   481\n",
       "SQL                      413\n",
       "Cloud                    373\n",
       "Microsoft Excel          244\n",
       "Amazon Web Services      244\n",
       "Azure                    222\n",
       "Spark                    175\n",
       "Airflow                  150\n",
       "Databricks               134\n",
       "BI                       133\n",
       "Agile                    126\n",
       "Apache Kafka             117\n",
       "Snowflake                117\n",
       "Google Cloud Platform    107\n",
       "Terraform                102\n",
       "DevOps                   102\n",
       "Docker                    89\n",
       "Orchestration             89\n",
       "ML                        87\n",
       "Kubernetes                85\n",
       "Java                      83\n",
       "PostgreSQL                82\n",
       "Google BigQuery           78\n",
       "Streaming                 74\n",
       "NoSQL                     69\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_skills.skills.value_counts().head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "a946840b-e27f-4eba-a9c5-b2ac831aab6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "skills\n",
       "QlikView              2\n",
       "MDM                   2\n",
       "Azure Blob Storage    2\n",
       "Columnar DBMS         1\n",
       "Azure Function        1\n",
       "DLT                   1\n",
       "SISTRA                1\n",
       "Power Query           1\n",
       "VBA                   1\n",
       "UC                    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_skills.skills.value_counts().tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "2cd556f7-364b-45f8-bf7a-a4562d826332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Google BigQuery', 'Google Cloud Platform'}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7f6f72ab-65f8-4af3-b88c-c94725afc98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Google BigQuery', 'Google Cloud Platform'},\n",
       " {'Azure',\n",
       "  'Azure Blob Storage',\n",
       "  'Azure Cosmos',\n",
       "  'Azure Data Factory',\n",
       "  'Azure Synapse Analytics'},\n",
       " {'Amazon Athena',\n",
       "  'Amazon Firehose',\n",
       "  'Amazon Glue',\n",
       "  'Amazon Redshift',\n",
       "  'Amazon S3',\n",
       "  'Amazon Web Services'})"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d571ba78-2c08-459a-98b0-b8d35a4b58bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
