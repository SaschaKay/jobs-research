{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051b11b-4488-4298-b542-3a09d8e2a90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104511/2850646582.py:14: UserWarning: __file__ was not available, os.getcwd() was used instead. You may need to change the working directory.\n",
      "  warnings.warn(CUR_DIR_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/repos/jobs-research was added to sys.path\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# adding paths for project modules\n",
    "CUR_DIR_WARNING = (\n",
    "    \"__file__ was not available, os.getcwd() was used instead. \"\n",
    "    \"You may need to change the working directory.\"\n",
    ")\n",
    "try:\n",
    "    CURRENT_DIRECTORY = os.path.dirname(__file__)\n",
    "except NameError:\n",
    "    CURRENT_DIRECTORY = os.getcwd()\n",
    "    warnings.warn(CUR_DIR_WARNING)\n",
    "\n",
    "if CURRENT_DIRECTORY not in sys.path:\n",
    "    sys.path.append(CURRENT_DIRECTORY)\n",
    "    \n",
    "from config import PROJECT_ROOT_RELATIVE\n",
    "PROJECT_ROOT = os.path.abspath(\n",
    "    os.path.join(CURRENT_DIRECTORY, PROJECT_ROOT_RELATIVE)\n",
    ")\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "    print(f\"{PROJECT_ROOT} was added to sys.path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d161945-4cb0-4474-b587-8ce06fb52ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "import re\n",
    "from math import ceil\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from common.utils import (\n",
    "    df_to_bq,\n",
    "    bq_table_to_df,\n",
    "    bq_merge,\n",
    "    print_dict,\n",
    ")\n",
    "from functions import (\n",
    "    get_post_id,\n",
    "    LoadsLogger,\n",
    ")\n",
    "from mappings import (\n",
    "    POSITIONS, \n",
    "    CITY_CLUSTERS,\n",
    "    find_position_in_text, \n",
    "    collapse_city_groups, \n",
    "    prepare_mapping_dict,\n",
    ")\n",
    "from config import (\n",
    "    PRINT_SQL,\n",
    "    JOBS_POSTINGS_FINAL_COLS,\n",
    "    BQ_PARAMS,\n",
    "    GCP_NAME,\n",
    "    SERVER, #switching between test/prod parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "789085f7-579c-42af-bd41-9fb8fcfcbc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client()\n",
    "source_tables_prefix = f\"{GCP_NAME[SERVER]}.{BQ_PARAMS[SERVER]['dataset_name']}.\"\n",
    "dataset = BQ_PARAMS[SERVER]['dataset_name']\n",
    "project = GCP_NAME[SERVER]\n",
    "pipeline_name = \"jobs_posting_transform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c859ee-10e9-40bc-90d1-759181e64ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch new data\n",
    "# deal with doubled posts\n",
    "# normalize attributes\n",
    "# update analytical tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddb68c3d-62bd-48e4-a21d-f1da5748ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------fetch new data------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0dd0bb3-9ff5-4139-a2ed-0efeef62c592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "with processed_loads as (\n",
      "  select dlt_load_id\n",
      "  from `x-avenue-450615-c3.job_postings_test._jp_processed_loads`\n",
      "  where processed_by = 'jobs_posting_transform'\n",
      "  group by dlt_load_id\n",
      "  having max(finished_at) is not Null\n",
      ")\n",
      "\n",
      ", new_loads as (\n",
      "  select distinct load_id\n",
      "  from `x-avenue-450615-c3.job_postings_test.._dlt_loads` as dl\n",
      "  left join processed_loads as pl on dl.load_id = pl.dlt_load_id\n",
      "  where dl.status = 0\n",
      "    and pl.dlt_load_id is Null\n",
      ")\n",
      "\n",
      "select\n",
      "    _dlt_load_id\n",
      "    ,_dlt_id\n",
      "    ,company\n",
      "    ,city\n",
      "    ,title\n",
      "    ,occupation\n",
      "    ,url\n",
      "    ,portal\n",
      "    ,experience_requirements__months_of_experience\n",
      "    ,date_created\n",
      "    ,description \n",
      "from `x-avenue-450615-c3.job_postings_test..jobs_posting` as jp\n",
      "inner join new_loads nl on jp._dlt_load_id = nl.load_id\n",
      "where locale = \"en_DE\"\n",
      "\n",
      "Fetched 173 raws from `x-avenue-450615-c3.job_postings_test..jobs_posting`\n"
     ]
    }
   ],
   "source": [
    "df_posting_load_query = f\"\"\"\n",
    "with processed_loads as (\n",
    "  select dlt_load_id\n",
    "  from `{source_tables_prefix}_jp_processed_loads`\n",
    "  where processed_by = '{pipeline_name}'\n",
    "  group by dlt_load_id\n",
    "  having max(finished_at) is not Null\n",
    ")\n",
    "\n",
    ", new_loads as (\n",
    "  select distinct load_id\n",
    "  from `{source_tables_prefix}._dlt_loads` as dl\n",
    "  left join processed_loads as pl on dl.load_id = pl.dlt_load_id\n",
    "  where dl.status = 0\n",
    "    and pl.dlt_load_id is Null\n",
    ")\n",
    "\n",
    "select\n",
    "    _dlt_load_id\n",
    "    ,_dlt_id\n",
    "    ,company\n",
    "    ,city\n",
    "    ,title\n",
    "    ,occupation\n",
    "    ,url\n",
    "    ,portal\n",
    "    ,experience_requirements__months_of_experience\n",
    "    ,date_created\n",
    "    ,description \n",
    "from `{source_tables_prefix}.jobs_posting` as jp\n",
    "inner join new_loads nl on jp._dlt_load_id = nl.load_id\n",
    "where locale = \"en_DE\"\n",
    "\"\"\"\n",
    "if PRINT_SQL:\n",
    "    print(df_posting_load_query)\n",
    "df_posting = bq_client.query(df_posting_load_query).to_dataframe()\n",
    "print(f\"Fetched {len(df_posting)} raws from `{source_tables_prefix}.jobs_posting`\")\n",
    "\n",
    "if df_posting.empty:\n",
    "    print(\"No data to process\")\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6650c84-8337-48ae-acb2-faed424a4583",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loads = LoadsLogger(df_posting, dataset, project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9212dfe-c580-432a-9daa-e4780f3a44ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 rows to x-avenue-450615-c3.job_postings_test._jp_processed_loads\n"
     ]
    }
   ],
   "source": [
    "new_loads.start(pipeline_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abd33014-19b6-4016-9715-c089400abe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posting.drop(columns=\"_dlt_load_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c026cf-5a0b-4833-937b-efa18d27766d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63ea1a79-210d-497c-b61b-4caa18c43916",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 68\u001b[0m\n\u001b[1;32m     59\u001b[0m df_posting\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle_lower_no_spaces\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moccupation_lower_no_spaces\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moccupation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m ], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m#----------------------------------------------------normalize cities------------------------------------------------\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m df_posting[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcity_group\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_posting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcity\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollapse_city_groups\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCITY_CLUSTERS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m df_posting\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m\"\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     71\u001b[0m df_posting[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myears_of_experience\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m(df_posting[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperience_requirements__months_of_experience\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     72\u001b[0m                                        \u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(x) \u001b[38;5;28;01melse\u001b[39;00m ceil(x\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m     73\u001b[0m                                     )\n\u001b[1;32m     74\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/series.py:4544\u001b[0m, in \u001b[0;36mSeries.map\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   4464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\n\u001b[1;32m   4465\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4466\u001b[0m     arg: Callable \u001b[38;5;241m|\u001b[39m Mapping \u001b[38;5;241m|\u001b[39m Series,\n\u001b[1;32m   4467\u001b[0m     na_action: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4468\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m   4469\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4470\u001b[0m \u001b[38;5;124;03m    Map values of Series according to an input mapping or function.\u001b[39;00m\n\u001b[1;32m   4471\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4542\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[1;32m   4543\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4544\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_values, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[1;32m   4546\u001b[0m         \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4547\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 68\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     59\u001b[0m df_posting\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle_lower_no_spaces\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moccupation_lower_no_spaces\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moccupation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m ], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m#----------------------------------------------------normalize cities------------------------------------------------\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m df_posting[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcity_group\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_posting\u001b[38;5;241m.\u001b[39mcity\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mcollapse_city_groups\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCITY_CLUSTERS\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     69\u001b[0m df_posting\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m\"\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     71\u001b[0m df_posting[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myears_of_experience\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m(df_posting[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperience_requirements__months_of_experience\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     72\u001b[0m                                        \u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(x) \u001b[38;5;28;01melse\u001b[39;00m ceil(x\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m     73\u001b[0m                                     )\n\u001b[1;32m     74\u001b[0m )\n",
      "File \u001b[0;32m~/work/repos/jobs-research/pipelines/rapidapi_jobs_posting/mappings.py:137\u001b[0m, in \u001b[0;36mcollapse_city_groups\u001b[0;34m(city_name, city_clusters)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(city_name, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOther\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 137\u001b[0m city_lc \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[^a-zA-Z]+\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, city_name)\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m region, keywords \u001b[38;5;129;01min\u001b[39;00m city_clusters\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[^a-zA-Z]+\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, keyword) \u001b[38;5;129;01min\u001b[39;00m city_lc \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m keywords):\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------deal with doubled posts------------------------------------------------\n",
    "\n",
    "#consider posts with the same title, description, location, and hiring company the same\n",
    "df_posting[\"job_id\"] = df_posting[[\"title\", \"company\", \"city\", \"description\"]].apply(get_post_id, axis=1, raw=True)\n",
    "\n",
    "#marking the last post, only this one will go to the analytical table\n",
    "df_posting.sort_values(\n",
    "    [\"job_id\", \"date_created\"], \n",
    "    ascending = False, \n",
    "    inplace = True\n",
    ")\n",
    "df_posting[\"is_source\"] = df_posting.groupby(by=\"job_id\").cumcount()==0\n",
    "\n",
    "#save mapping from old id on new\n",
    "df_dlt_to_post_id = df_posting[[\"_dlt_id\", \"job_id\", \"is_source\"]].copy(deep = True)\n",
    "\n",
    "#get rid of doubles\n",
    "df_posting = df_posting[df_posting.is_source].copy()\n",
    "\n",
    "df_posting.drop(columns=['_dlt_id', 'is_source'], inplace = True)\n",
    "\n",
    "#----------------------------------------------------normalize attributes------------------------------------------------\n",
    "\n",
    "#preparing fields for mapping attributes\n",
    "df_posting[\"title_lower_no_spaces\"] = df_posting.title.map(\n",
    "    lambda x: x.lower().replace(\" \", \"\")\n",
    ")\n",
    "df_posting[\"occupation_lower_no_spaces\"] = df_posting.occupation.map(\n",
    "    lambda x: x.lower().replace(\" \", \"\")\n",
    ")\n",
    "\n",
    "#preparing mapping rules\n",
    "map_dicts_positions_prepared = [\n",
    "    prepare_mapping_dict(*mapping_dict) for mapping_dict in POSITIONS\n",
    "]\n",
    "\n",
    "#----------------------------------------------------normalize positions------------------------------------------------\n",
    "\n",
    "df_posting[\"position\"] = None\n",
    "\n",
    "for md in map_dicts_positions_prepared:\n",
    "    if not (md.case_sensitive & md.spaces_sensitive):\n",
    "        text_columns = [\"title_lower_no_spaces\", \"occupation_lower_no_spaces\"]\n",
    "    elif md.case_sensitive & md.spaces_sensitive:\n",
    "        text_columns = [\"title\", \"occupation\"]\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"You need a small refinement to use case_sensitive != spaces_sensitive\"\n",
    "        )\n",
    "    df_posting[\"position\"] = df_posting[[\"position\", *text_columns]].apply(\n",
    "        lambda x: (\n",
    "            x.iloc[0]\n",
    "            if x.iloc[0] is not None\n",
    "            else find_position_in_text(x.iloc[1:], md.mapping_dict)\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    \n",
    "df_posting.drop(columns=[\n",
    "    \"title_lower_no_spaces\",\n",
    "    \"occupation_lower_no_spaces\",\n",
    "    \"title\",\n",
    "    \"occupation\"\n",
    "], inplace=True)\n",
    "\n",
    "#----------------------------------------------------normalize cities------------------------------------------------\n",
    "\n",
    "df_posting[\"city_group\"] = df_posting.city.map(lambda x: collapse_city_groups(x, CITY_CLUSTERS))\n",
    "df_posting.drop(columns=\"city\", inplace=True)\n",
    "\n",
    "df_posting['years_of_experience']=(df_posting['experience_requirements__months_of_experience']\n",
    "                                       .map(lambda x: None if pd.isna(x) else ceil(x/12)\n",
    "                                    )\n",
    ")\n",
    "df_posting.drop(columns=\"experience_requirements__months_of_experience\", inplace=True)\n",
    "\n",
    "\n",
    "#----------------------------------------------------update analytical tables------------------------------------------------\n",
    "\n",
    "#download data to the tmp table\n",
    "jobs_columns = list(JOBS_POSTINGS_FINAL_COLS.keys())\n",
    "df_posting.rename(columns = {\"job_id\": \"id\"}, inplace=True)\n",
    "df_posting = df_posting[jobs_columns]\n",
    "df_to_bq(df_posting, '_jp_jobs_batch', dataset, project, truncate=True)\n",
    "\n",
    "# save info about new ids \n",
    "df_dlt_to_post_id.rename(columns = {\"_dlt_id\": \"dlt_id\"}, inplace=True)\n",
    "df_dlt_to_post_id['matched_at'] = dt.datetime.now()\n",
    "df_to_bq(df_dlt_to_post_id, '_jp_dlt_ids_matching', dataset, project, truncate=False)\n",
    "\n",
    "#update main analytical table\n",
    "bq_merge(\n",
    "    f\"{project}.jp.jobs\",\n",
    "    f\"{project}.{dataset}._jp_jobs_batch\", \n",
    "    \"id\",\n",
    "    jobs_columns[1:], #exclude key column\n",
    "    print_sql = PRINT_SQL,\n",
    ")\n",
    "\n",
    "# log\n",
    "new_loads.update_status(pipeline_name, dataset, project)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9474f0a3-dc79-465a-8ac1-d041e8566682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
