{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e051b11b-4488-4298-b542-3a09d8e2a90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115761/2850646582.py:14: UserWarning: __file__ was not available, os.getcwd() was used instead. You may need to change the working directory.\n",
      "  warnings.warn(CUR_DIR_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/repos/jobs-research was added to sys.path\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# adding paths for project modules\n",
    "CUR_DIR_WARNING = (\n",
    "    \"__file__ was not available, os.getcwd() was used instead. \"\n",
    "    \"You may need to change the working directory.\"\n",
    ")\n",
    "try:\n",
    "    CURRENT_DIRECTORY = os.path.dirname(__file__)\n",
    "except NameError:\n",
    "    CURRENT_DIRECTORY = os.getcwd()\n",
    "    warnings.warn(CUR_DIR_WARNING)\n",
    "\n",
    "if CURRENT_DIRECTORY not in sys.path:\n",
    "    sys.path.append(CURRENT_DIRECTORY)\n",
    "    \n",
    "from config import PROJECT_ROOT_RELATIVE\n",
    "PROJECT_ROOT = os.path.abspath(\n",
    "    os.path.join(CURRENT_DIRECTORY, PROJECT_ROOT_RELATIVE)\n",
    ")\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "    print(f\"{PROJECT_ROOT} was added to sys.path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d161945-4cb0-4474-b587-8ce06fb52ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from common.utils import (\n",
    "    df_to_bq,\n",
    "    bq_table_to_df,\n",
    "    bq_merge,\n",
    "    print_dict,\n",
    ")\n",
    "from functions import (\n",
    "    get_post_id,\n",
    "    LoadsLogger,\n",
    ")\n",
    "from mappings import (\n",
    "    POSITIONS, \n",
    "    CITY_CLUSTERS,\n",
    "    find_position_in_text, \n",
    "    collapse_city_groups, \n",
    "    prepare_mapping_dict,\n",
    ")\n",
    "from config import (\n",
    "    PRINT_SQL,\n",
    "    JOBS_POSTINGS_FINAL_COLS,\n",
    "    BQ_DWH_PARAMS,\n",
    "    BQ_ADB_PARAMS,\n",
    "    GCP_NAME,\n",
    "    SERVER, #switching between test/prod parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "789085f7-579c-42af-bd41-9fb8fcfcbc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = BQ_DWH_PARAMS[SERVER]['location']\n",
    "bq_client = bigquery.Client(location=location)\n",
    "source_tables_prefix = f\"{GCP_NAME[SERVER]}.{BQ_DWH_PARAMS[SERVER]['dataset_name']}.\"\n",
    "dataset = BQ_DWH_PARAMS[SERVER]['dataset_name']\n",
    "analytical_dataset = BQ_ADB_PARAMS[SERVER]['dataset_name']\n",
    "project = GCP_NAME[SERVER]\n",
    "pipeline_name = \"jobs_posting_transform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62c859ee-10e9-40bc-90d1-759181e64ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch new data\n",
    "# deal with doubled posts\n",
    "# normalize attributes\n",
    "# update analytical tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb68c3d-62bd-48e4-a21d-f1da5748ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------fetch new data------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0dd0bb3-9ff5-4139-a2ed-0efeef62c592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "with processed_loads as (\n",
      "  select dlt_load_id\n",
      "  from `x-avenue-450615-c3.job_postings_test._jp_processed_loads`\n",
      "  where processed_by = 'jobs_posting_transform'\n",
      "  group by dlt_load_id\n",
      "  having max(finished_at) is not Null\n",
      ")\n",
      "\n",
      ", new_loads as (\n",
      "  select distinct load_id\n",
      "  from `x-avenue-450615-c3.job_postings_test.._dlt_loads` as dl\n",
      "  left join processed_loads as pl on dl.load_id = pl.dlt_load_id\n",
      "  where dl.status = 0\n",
      "    and pl.dlt_load_id is Null\n",
      ")\n",
      "\n",
      "select\n",
      "    _dlt_load_id\n",
      "    ,_dlt_id\n",
      "    ,company\n",
      "    ,city\n",
      "    ,title\n",
      "    ,occupation\n",
      "    ,url\n",
      "    ,portal\n",
      "    ,experience_requirements__months_of_experience\n",
      "    ,date_created\n",
      "    ,description \n",
      "from `x-avenue-450615-c3.job_postings_test..jobs_posting` as jp\n",
      "inner join new_loads nl on jp._dlt_load_id = nl.load_id\n",
      "where locale = \"en_DE\"\n",
      "\n",
      "Fetched 0 raws from `x-avenue-450615-c3.job_postings_test..jobs_posting`\n",
      "No data to process\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "df_posting_load_query = f\"\"\"\n",
    "with processed_loads as (\n",
    "  select dlt_load_id\n",
    "  from `{source_tables_prefix}_jp_processed_loads`\n",
    "  where processed_by = '{pipeline_name}'\n",
    "  group by dlt_load_id\n",
    "  having max(finished_at) is not Null\n",
    ")\n",
    "\n",
    ", new_loads as (\n",
    "  select distinct load_id\n",
    "  from `{source_tables_prefix}._dlt_loads` as dl\n",
    "  left join processed_loads as pl on dl.load_id = pl.dlt_load_id\n",
    "  where dl.status = 0\n",
    "    and pl.dlt_load_id is Null\n",
    ")\n",
    "\n",
    "select\n",
    "    _dlt_load_id\n",
    "    ,_dlt_id\n",
    "    ,company\n",
    "    ,city\n",
    "    ,title\n",
    "    ,occupation\n",
    "    ,url\n",
    "    ,portal\n",
    "    ,experience_requirements__months_of_experience\n",
    "    ,date_created\n",
    "    ,description \n",
    "from `{source_tables_prefix}.jobs_posting` as jp\n",
    "inner join new_loads nl on jp._dlt_load_id = nl.load_id\n",
    "where locale = \"en_DE\"\n",
    "\"\"\"\n",
    "if PRINT_SQL:\n",
    "    print(df_posting_load_query)\n",
    "df_posting = bq_client.query(df_posting_load_query).to_dataframe()\n",
    "print(f\"Fetched {len(df_posting)} raws from `{source_tables_prefix}.jobs_posting`\")\n",
    "\n",
    "if df_posting.empty:\n",
    "    print(\"No data to process\")\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6650c84-8337-48ae-acb2-faed424a4583",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loads = LoadsLogger(df_posting, dataset, project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9212dfe-c580-432a-9daa-e4780f3a44ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loads.start(pipeline_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd33014-19b6-4016-9715-c089400abe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posting.drop(columns=\"_dlt_load_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c026cf-5a0b-4833-937b-efa18d27766d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ea1a79-210d-497c-b61b-4caa18c43916",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------deal with doubled posts------------------------------------------------\n",
    "\n",
    "#consider posts with the same title, description, location, and hiring company the same\n",
    "df_posting[\"job_id\"] = df_posting[[\"title\", \"company\", \"city\", \"description\"]].apply(get_post_id, axis=1, raw=True)\n",
    "\n",
    "#marking the last post, only this one will go to the analytical table\n",
    "df_posting.sort_values(\n",
    "    [\"job_id\", \"date_created\"], \n",
    "    ascending = False, \n",
    "    inplace = True\n",
    ")\n",
    "df_posting[\"is_source\"] = df_posting.groupby(by=\"job_id\").cumcount()==0\n",
    "\n",
    "#save mapping from old id on new\n",
    "df_dlt_to_post_id = df_posting[[\"_dlt_id\", \"job_id\", \"is_source\"]].copy(deep = True)\n",
    "\n",
    "#get rid of doubles\n",
    "df_posting = df_posting[df_posting.is_source].copy()\n",
    "\n",
    "df_posting.drop(columns=['_dlt_id', 'is_source'], inplace = True)\n",
    "\n",
    "#----------------------------------------------------normalize attributes------------------------------------------------\n",
    "\n",
    "#preparing fields for mapping attributes\n",
    "df_posting[\"title_lower_no_spaces\"] = df_posting.title.map(\n",
    "    lambda x: x.lower().replace(\" \", \"\")\n",
    ")\n",
    "df_posting[\"occupation_lower_no_spaces\"] = df_posting.occupation.map(\n",
    "    lambda x: x.lower().replace(\" \", \"\")\n",
    ")\n",
    "\n",
    "#preparing mapping rules\n",
    "map_dicts_positions_prepared = [\n",
    "    prepare_mapping_dict(*mapping_dict) for mapping_dict in POSITIONS\n",
    "]\n",
    "\n",
    "#----------------------------------------------------normalize positions------------------------------------------------\n",
    "\n",
    "df_posting[\"position\"] = None\n",
    "\n",
    "for md in map_dicts_positions_prepared:\n",
    "    if not (md.case_sensitive & md.spaces_sensitive):\n",
    "        text_columns = [\"title_lower_no_spaces\", \"occupation_lower_no_spaces\"]\n",
    "    elif md.case_sensitive & md.spaces_sensitive:\n",
    "        text_columns = [\"title\", \"occupation\"]\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"You need a small refinement to use case_sensitive != spaces_sensitive\"\n",
    "        )\n",
    "    df_posting[\"position\"] = df_posting[[\"position\", *text_columns]].apply(\n",
    "        lambda x: (\n",
    "            x.iloc[0]\n",
    "            if x.iloc[0] is not None\n",
    "            else find_position_in_text(x.iloc[1:], md.mapping_dict)\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    \n",
    "df_posting.drop(columns=[\n",
    "    \"title_lower_no_spaces\",\n",
    "    \"occupation_lower_no_spaces\",\n",
    "    \"title\",\n",
    "    \"occupation\"\n",
    "], inplace=True)\n",
    "\n",
    "#----------------------------------------------------normalize cities------------------------------------------------\n",
    "\n",
    "df_posting[\"city_group\"] = df_posting.city.map(lambda x: collapse_city_groups(x, CITY_CLUSTERS))\n",
    "df_posting.drop(columns=\"city\", inplace=True)\n",
    "\n",
    "df_posting['years_of_experience']=(df_posting['experience_requirements__months_of_experience']\n",
    "                                       .map(lambda x: None if pd.isna(x) else ceil(x/12)\n",
    "                                    )\n",
    ")\n",
    "df_posting.drop(columns=\"experience_requirements__months_of_experience\", inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27945f43-49ae-42a9-8b3d-3114a687aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------update analytical tables------------------------------------------------\n",
    "\n",
    "#download data to the tmp table\n",
    "jobs_columns = list(JOBS_POSTINGS_FINAL_COLS.keys())\n",
    "df_posting.rename(columns = {\"job_id\": \"id\"}, inplace=True)\n",
    "df_posting = df_posting[jobs_columns]\n",
    "df_to_bq(df_posting, '_jp_jobs_batch', dataset, project, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9474f0a3-dc79-465a-8ac1-d041e8566682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save info about new ids \n",
    "df_dlt_to_post_id.rename(columns = {\"_dlt_id\": \"dlt_id\"}, inplace=True)\n",
    "df_dlt_to_post_id['matched_at'] = dt.datetime.now()\n",
    "df_to_bq(df_dlt_to_post_id, '_jp_dlt_ids_matching', dataset, project, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901e2f7e-16b3-419a-8083-fabafe75225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update main analytical table\n",
    "bq_merge(\n",
    "    f\"{project}.{analytical_dataset}.jobs\",\n",
    "    f\"{project}.{dataset}._jp_jobs_batch\", \n",
    "    \"id\",\n",
    "    jobs_columns[1:], #exclude key column\n",
    "    print_sql = PRINT_SQL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff5101a-27d5-4de7-81e1-5c69c08f5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log\n",
    "new_loads.finish(pipeline_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4a6026-a5dd-49c4-aa16-76386e9f76f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
